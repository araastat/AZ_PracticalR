
---
class: middle, center, inverse

# Data munging

---

## Data transformation (dplyr)

The `dplyr` package gives us a few verbs for data manipulation

```{r, echo = F, results='asis'}
dat <- tribble(
  ~Function, ~Purpose,
  'select', "Select columns based on name or position",
  'mutate', 'Create or change a column',
  'filter', 'Extract rows based on some criteria',
  'arrange', 'Re-order rows based on values of variable(s)',
  'group_by', 'Split a dataset by unique values of a variable',
  'summarize', 'Create summary statistics based on columns')
knitr::kable(dat, format='markdown')
```

---

## `select`

You can select columns by name or position, of course, e.g., `select(weather, month)` or `select(weather, 3)`

You can select consecutive columns using `:` notation, e.g. `select(weather, d1:d31)`

You can also select columns based on some criteria, which are encapsulated in functions.

- `starts_with("___")`, `ends_with("___")`, `contains("____")`
- `one_of("____","_____","______")`
- `everything()`

There are others; see `help(starts_with)`.

These selection methods work in all tidyverse functions

> Note that for `select` the names of the columns don't need to be quoted. This is called *non-standard evaluation* and
is a convenience. However for the criteria-based selectors within `select`, you **do** need to quote the criteria

---

## select

```{r}
weather_data <- rio::import('../../data/weather.csv')
head(weather_data)
```

---

## select

```{r munging-24, eval = T, echo = T}

weather1 <- select(weather_data, year, month, d1:d31) 
head(weather1)
```

---

## select

```{r, eval = T, echo = T}
weather1 <- select(weather_data, starts_with('d')) 
head(weather1)
```


---

## select

The flexibility of the `select` function, and others we'll see presently, is quite powerful. 

Suppose you have a large genomic data where the columns are different genes, and suppose that the housekeeping genes all start with "HK". Then, in order to _remove_ the housekeeping genes, you could just do

```{r, eval=F}
new_data <- select(old_data, -starts_with("HK"))
```

Here, the `-` sign means, remove those columns.

Also note that we have to assign the selected dataset to a new (or old) name in order to 
preserve it in the workspace.

---

## select

I always prefer naming my columns well and using the capabilities of `select` to grab columns. 

However, you can use `select` with column numbers. For example, if you wanted to grab the 
first 4 columns of a dataset, you could do

```{r, eval=F}
new_data <- select(old_data, 1:4)
```

.footnote[The notation `1:4` is a short hand for the sequence `1,2,3,4`. Generally, the notation `m:n` means the set of consecutive integers between `m` and `n`.]

---

## Exercise

1. Load the BreastCancer_Expression_full.csv file into R, naming it `brca_expression`.
1. Create a new dataset from `brca_expression` that contains columns with names starting with "NP_0019"
1. Create a second dataset from `brca_expression` that contains the column 'TCGA_ID' and columns starting with "NP_00200".
1. Take this last dataset and convert it's column names to lower case using `janitor::clean_names`

```{r, echo=F}
countdown(minutes=10)
```


---

## mutate

`mutate`, as the name suggests, either creates a new column in your data set or transforms an existing column.

We'll work with the `mpg` dataset that is included in the **ggplot2** package. This is a fuel economy dataset from the US EPA. There are two columns, `hwy` and `cty` which give the highway and city fuel efficiency in miles per gallon. 

For our non-American friends, we will convert these columns to kilometres per litre. 

.pull-left[
```{r m1, eval = F, echo = T}
mpg_si <- mutate(mpg, 
                 hwy_si = hwy * 1.6 / 3.8,
                 cty_si = cty * 1.6 / 3.8)
str(mpg_si)
```

This creates new columns `hwy_si` and `cty_si`
]
.pull-right[
```{r, eval=T, echo = F, ref.label="m1"}
```
]
---

## mutate

`mutate`, as the name suggests, either creates a new column in your data set or transforms an existing column.

We'll work with the `mpg` dataset that is included in the **ggplot2** package. This is a fuel economy dataset from the US EPA. There are two columns, `hwy` and `cty` which give the highway and city fuel efficiency in miles per gallon. 

For our non-American friends, we will convert these columns to kilometres per litre. 

.pull-left[
```{r m2, eval = F, echo = T}
mpg <- mutate(mpg, 
              hwy = hwy * 1.6 / 3.8,
              cty = cty * 1.6 / 3.8)
str(mpg)
```

This changes the original data in place, and replaces the columns with the transformed data
]
.pull-right[
```{r, eval=T, echo = F, ref.label="m2"}
```
]

---

## across

**dplyr** version 1.0 introduced a new verb, `across` to allow functions like `mutate` (and `summarize`, which we shall see in the statistics module) to act on a selection of columns 
which can be chosen using the same syntax as `select`, or by condition. The `mutate` operation changes variables **in place** in this paradigm

.pull-left[
```{r, eval=F}
mutate(mpg, 
       cty = cty * 1.6/3.8,
       hwy = hwy * 1.6/3.8)
```

]
.pull-right[
```{r, echo=T, eval=F}
mutate(mpg, 
       across(c(cty, hwy), 
                   function(x) {x * 1.6/3.8}))
```
]

-----

```{r,eval=F}
mutate(mpg, 
       across(where(is.character), as.factor)) # select based on condition
```

-----

---

## Exercise

The `across` concept is quite powerful. We'll explore it a bit more using the **palmerpenguins** package.

1. Install the **palmerpenguins** package using `remotes::install_github('allisonhorst/palmerpenguins')`
1. Activate the library using `library(palmerpenguins)`
1. Display the first few rows of the data using `head()`
1. You'll notice that there are missing values in some columns. Let's replace these with the column means.

```{r, echo=F}
countdown(minutes=10)
```

---

## filter

`filter` extracts **rows** based on criteria. 

Some comparison operators for filtering

| Operator | Meaning                          |
|----------|----------------------------------|
| ==       | Equals                           |
| !=       | Not equals                       |
| > / <    | Greater / less than              |
| >= / <=  | Greater or equal / Less or equal |
| !=       | Not equal                        |
| %in%     | In a set                         |
| is.na()  | is a missing value |
| !is.na() | not a missing value|

Combining comparisons

| Operator   | Meaning |
|------------|---------|
| &          | And     |
| &#124;       | Or      |

---

## filter

Some comparison operators for filtering

Strings: `str_detect(<variable>, "<string>")` or `str_detect(<variable>, "<regex>")`

Regex or regular expression basics:

```{r, echo=F}
tribble(~Expression, ~Meaning,
        '[a,b,c]', 'Matches "a", "b" or "c"',
        '[a-z]', 'Matches letters between "a" and "z"',
        '[^abc]', 'Matches anything except "a", "b" and "c"',
        "[:alpha:]", "letters",
        "[:digit:]", "digits",
        "[:alnum:]", "letters or numbers",
        "[:punct:]", "punctuation") %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

.footnote[Many more details are available [here](https://stringr.tidyverse.org/articles/regular-expressions.html#special-characters-1) and a cheatsheet is available [here](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf)]

---

## filter

The `filter` function extracts rows that meet logical criteria.

.pull-left[
```{r, eval=FALSE}
filter(.data, ...)
```
]
.pull-right[
+ `.data` is the data frame to transform
+ `...` represents one or more logical tests (returning rows for which the test is TRUE)
]

---

## filter

Let's use the `penguins` dataset again. We want to extract the **rows** with data for male Adelie penguins

.pull-left[
```{r mung0, eval = F, echo = T}
filter(penguins, 
       (species == 'Adelie') & 
         (sex == 'male'))
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="mung0"}
```
]

---

## filter

Let's extract rows with data from Biscoe island where the penguins are either below 3000 g or above 5000 g in weight.

.pull-left[
```{r mung1, eval = F, echo = T}
filter(penguins, (island == 'Biscoe') & 
  ((body_mass_g < 3000) | (body_mass_g > 5000)))
```

I'm a big fan of using parentheses to specify the individual
comparisons in a complex query like we're doing here. 
]
.pull-right[
```{r, eval=T, echo = F, ref.label="mung1"}
```
]

---

## filter

A common use of `filter` is to remove rows with missing values from your dataset

.pull-left[
```{r mung2, eval = F, echo = T}
penguins_complete <- filter(penguins, 
                            !is.na(bill_length_mm))
head(penguins_complete)
```

`is.na` is a *function* that tests whether a value is missing or not. 

So `!is.na` is the opposite of that. 

> The condition that you are using in `filter` is what you want **to keep**.
]
.pull-right[
```{r, eval=T, echo = F, ref.label="mung2"}
```
]

---

## filter

### Two common mistakes

1. Using .heatinline[`=`] instead of .saltinline[`==`]
```{r, eval=F}
filter(penguins, species = 'Adelie')
filter(penguins, species == 'Adelie')
```

2. Forgetting quotes
```{r, eval=F}
filter(penguins, species == Adelie)
filter(penguins, species == "Adelie")
```


---

## Important distinction

.pull-left[
.acid[The `filter` function affects **rows** of a dataset]
]
.pull-right[
.heat[The `select` function affects **columns** of a dataset]
]

---

## slice

You can use `slice` and siblings to subset **rows** of a data set by index. 

+ `slice(mpg, 1,2,5)` grabs rows 1, 2 and 5
+ `slice_head(mpg)` / `slice_tail(mpg)` grabs first/last row of data set
    - You can specify an argument `n` for the number of rows to grab
    - You can specify an argument `prop` for the proportion of rows to grab
+ `slice_sample(mpg, 10)` grabs 10 rows at random, without replacement
+ `slice_min(mpg, hwy)` / `slice_max(mpg, hwy)` gives the `n`/`prop` rows with the lowest/highest values of `hwy`.

---

## Exercise

1. Extract the rows from `penguins` with the smallest 10% of penguins by mass
1. Extract a random 20% of rows from `penguins`

```{r, echo=F}
countdown(minutes=5)
```


---
class: middle, center, inverse

# Workflow pipes in the tidyverse

---
  
## Pipes

Pipes are a method in R to create analytic pipelines utilizing tidyverse functions.

The pipe operator (denoted `%>%`, spoken as "then") is what creates the pipes.

You start with a dataset, and then progressively add functions to the pipe. Typically you save the result to a new object.

Each element of the pipe takes as its first argument the results of the previous step, which typically is a data frame.

Pipes are just a different representation of an analytic process that we can do in separate steps anyway. 

.footnote[The keyboard shortcut for the pipe operator in RStudio is .heatinline[`Ctrl/Cmd + Shift + m`]]

---

## Pipes

.pull-left[
Without pipes

```{r }
mpg1 <- mutate(mpg, id = 1:n())
mpg2 <- select(mpg1, id, year, trans, cty, hwy)
mpg_final <- mutate(mpg2, 
                    across(c(cty, hwy), 
                           function(x) {x * 1.6/3.8}))
```

]
.pull-right[
With pipes

```{r }
mpg_final <- mpg %>% 
  mutate(id = 1:n()) %>% 
  select(id, year, trans, cty, hwy) %>% 
  mutate(across(c(cty, hwy), 
                function(x){x*1.6/3.8}))
```

]

The important things to note here are:

1. When using pipes, the results of one operation are automatically entered into the **first argument** of the next function, so the actual specification omits the first argument
1. If you need the results of one step to go to some other argument of the next function, you can represent that input by `.`, for example, .fatinline[`mpg %>% lm(cty ~ hwy, data = .)`] takes the dataset `mpg` and places it in the argument for `data` in the `lm` function. 

---

## Exercise

<ol>
<li>Create a pipe that 
<ol type='a'>
  <li> starts with the _penguins_ dataset,
  <li> extracts the species, bill length and body mass columns, 
  <li> filters out rows with missing body mass measurements, and
  <li> creates a new columns with mass in pounds (1 lb = 453 grams) 
</ol>
</ol>
    
```{r, echo=F}
countdown(minutes=5)
```

