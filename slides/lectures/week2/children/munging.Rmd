
---
class: middle, center, inverse

# Data munging

---

## Data transformation (dplyr)

The `dplyr` package gives us a few verbs for data manipulation

```{r, echo = F, results='asis'}
dat <- tribble(
  ~Function, ~Purpose,
  'select', "Select columns based on name or position",
  'mutate', 'Create or change a column',
  'filter', 'Extract rows based on some criteria',
  'arrange', 'Re-order rows based on values of variable(s)',
  'group_by', 'Split a dataset by unique values of a variable',
  'summarize', 'Create summary statistics based on columns')
knitr::kable(dat, format='markdown')
```

---

## `select`

You can select columns by name or position, of course, e.g., `select(weather, month)` or `select(weather, 3)`

You can select consecutive columns using `:` notation, e.g. `select(weather, d1:d31)`

You can also select columns based on some criteria, which are encapsulated in functions.

- `starts_with("___")`, `ends_with("___")`, `contains("____")`
- `one_of("____","_____","______")`
- `everything()`

There are others; see `help(starts_with)`.

These selection methods work in all tidyverse functions

> Note that for `select` the names of the columns don't need to be quoted. This is called *non-standard evaluation* and
is a convenience. However for the criteria-based selectors within `select`, you **do** need to quote the criteria

---

## select

```{r}
weather_data <- rio::import('../../data/weather.csv')
head(weather_data)
```

---

## select

```{r munging-24, eval = T, echo = T}

weather1 <- select(weather_data, year, month, d1:d31) 
head(weather1)
```

---

## select

```{r, eval = T, echo = T}
weather1 <- select(weather_data, starts_with('d')) 
head(weather1)
```


---

## select

The flexibility of the `select` function, and others we'll see presently, is quite powerful. 

Suppose you have a large genomic data where the columns are different genes, and suppose that the housekeeping genes all start with "HK". Then, in order to _remove_ the housekeeping genes, you could just do

```{r, eval=F}
new_data <- select(old_data, -starts_with("HK"))
```

Here, the `-` sign means, remove those columns.

Also note that we have to assign the selected dataset to a new (or old) name in order to 
preserve it in the workspace.

---

## select

I always prefer naming my columns well and using the capabilities of `select` to grab columns. 

However, you can use `select` with column numbers. For example, if you wanted to grab the 
first 4 columns of a dataset, you could do

```{r, eval=F}
new_data <- select(old_data, 1:4)
```

.footnote[The notation `1:4` is a short hand for the sequence `1,2,3,4`. Generally, the notation `m:n` means the set of consecutive integers between `m` and `n`.]

---

## Exercise

1. Load the BreastCancer_Expression_full.csv file into R, naming it `brca_expression`.
1. Create a new dataset from `brca_expression` that contains columns with names starting with "NP_0019"
1. Create a second dataset from `brca_expression` that contains the column 'TCGA_ID' and columns starting with "NP_00200".
1. Take this last dataset and convert it's column names to lower case using `janitor::clean_names`

```{r, echo=F}
countdown(minutes=10)
```


---

## mutate

`mutate`, as the name suggests, either creates a new column in your data set or transforms an existing column.

We'll work with the `mpg` dataset that is included in the **ggplot2** package. This is a fuel economy dataset from the US EPA. There are two columns, `hwy` and `cty` which give the highway and city fuel efficiency in miles per gallon. 

For our non-American friends, we will convert these columns to kilometres per litre. 

.pull-left[
```{r m1, eval = F, echo = T}
mpg_si <- mutate(mpg, 
                 hwy_si = hwy * 1.6 / 3.8,
                 cty_si = cty * 1.6 / 3.8)
str(mpg_si)
```

This creates new columns `hwy_si` and `cty_si`
]
.pull-right[
```{r, eval=T, echo = F, ref.label="m1"}
```
]
---

## mutate

`mutate`, as the name suggests, either creates a new column in your data set or transforms an existing column.

We'll work with the `mpg` dataset that is included in the **ggplot2** package. This is a fuel economy dataset from the US EPA. There are two columns, `hwy` and `cty` which give the highway and city fuel efficiency in miles per gallon. 

For our non-American friends, we will convert these columns to kilometres per litre. 

.pull-left[
```{r m2, eval = F, echo = T}
mpg <- mutate(mpg, 
              hwy = hwy * 1.6 / 3.8,
              cty = cty * 1.6 / 3.8)
str(mpg)
```

This changes the original data in place, and replaces the columns with the transformed data
]
.pull-right[
```{r, eval=T, echo = F, ref.label="m2"}
```
]

---

## across

**dplyr** version 1.0 introduced a new verb, `across` to allow functions like `mutate` (and `summarize`, which we shall see in the statistics module) to act on a selection of columns 
which can be chosen using the same syntax as `select`, or by condition. The `mutate` operation changes variables **in place** in this paradigm

.pull-left[
```{r, eval=F}
mutate(mpg, 
       cty = cty * 1.6/3.8,
       hwy = hwy * 1.6/3.8)
```

]
.pull-right[
```{r, echo=T, eval=F}
mutate(mpg, 
       across(c(cty, hwy), 
                   function(x) {x * 1.6/3.8}))
```
]

-----

```{r,eval=F}
mutate(mpg, 
       across(where(is.character), as.factor)) # select based on condition
```

-----

---

## Exercise

The `across` concept is quite powerful. We'll explore it a bit more using the **palmerpenguins** package.

1. Install the **palmerpenguins** package using `remotes::install_github('allisonhorst/palmerpenguins')`
1. Activate the library using `library(palmerpenguins)`
1. Display the first few rows of the data using `head()`
1. You'll notice that there are missing values in some columns. Let's replace these with the column means.

```{r, echo=F}
countdown(minutes=10)
```

---

## filter

`filter` extracts **rows** based on criteria. 

Some comparison operators for filtering

| Operator | Meaning                          |
|----------|----------------------------------|
| ==       | Equals                           |
| !=       | Not equals                       |
| > / <    | Greater / less than              |
| >= / <=  | Greater or equal / Less or equal |
| !=       | Not equal                        |
| %in%     | In a set                         |
| is.na()  | is a missing value |
| !is.na() | not a missing value|

Combining comparisons

| Operator   | Meaning |
|------------|---------|
| &          | And     |
| &#124;       | Or      |

---

## filter

Some comparison operators for filtering

Strings: `str_detect(<variable>, "<string>")` or `str_detect(<variable>, "<regex>")`

Regex or regular expression basics:

```{r, echo=F}
tribble(~Expression, ~Meaning,
        '[a,b,c]', 'Matches "a", "b" or "c"',
        '[a-z]', 'Matches letters between "a" and "z"',
        '[^abc]', 'Matches anything except "a", "b" and "c"',
        "[:alpha:]", "letters",
        "[:digit:]", "digits",
        "[:alnum:]", "letters or numbers",
        "[:punct:]", "punctuation") %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

.footnote[Many more details are available [here](https://stringr.tidyverse.org/articles/regular-expressions.html#special-characters-1) and a cheatsheet is available [here](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf)]

---

## filter

The `filter` function extracts rows that meet logical criteria.

.pull-left[
```{r, eval=FALSE}
filter(.data, ...)
```
]
.pull-right[
+ `.data` is the data frame to transform
+ `...` represents one or more logical tests (returning rows for which the test is TRUE)
]

---

## filter

Let's use the `penguins` dataset again. We want to extract the **rows** with data for male Adelie penguins

.pull-left[
```{r mung0, eval = F, echo = T}
filter(penguins, 
       (species == 'Adelie') & 
         (sex == 'male'))
```
]
.pull-right[
```{r, eval=T, echo = F, ref.label="mung0"}
```
]

---

## filter

Let's extract rows with data from Biscoe island where the penguins are either below 3000 g or above 5000 g in weight.

.pull-left[
```{r mung1, eval = F, echo = T}
filter(penguins, (island == 'Biscoe') & 
  ((body_mass_g < 3000) | (body_mass_g > 5000)))
```

I'm a big fan of using parentheses to specify the individual
comparisons in a complex query like we're doing here. 
]
.pull-right[
```{r, eval=T, echo = F, ref.label="mung1"}
```
]

---

## filter

A common use of `filter` is to remove rows with missing values from your dataset

.pull-left[
```{r mung2, eval = F, echo = T}
penguins_complete <- filter(penguins, 
                            !is.na(bill_length_mm))
head(penguins_complete)
```

`is.na` is a *function* that tests whether a value is missing or not. 

So `!is.na` is the opposite of that. 

> The condition that you are using in `filter` is what you want **to keep**.
]
.pull-right[
```{r, eval=T, echo = F, ref.label="mung2"}
```
]

---

## filter

### Two common mistakes

1. Using .heatinline[`=`] instead of .saltinline[`==`]
```{r, eval=F}
filter(penguins, species = 'Adelie')
filter(penguins, species == 'Adelie')
```

2. Forgetting quotes
```{r, eval=F}
filter(penguins, species == Adelie)
filter(penguins, species == "Adelie")
```


---

## Important distinction

.pull-left[
.acid[The `filter` function affects **rows** of a dataset]
]
.pull-right[
.heat[The `select` function affects **columns** of a dataset]
]

---

## slice

You can use `slice` and siblings to subset **rows** of a data set by index. 

+ `slice(mpg, 1,2,5)` grabs rows 1, 2 and 5
+ `slice_head(mpg)` / `slice_tail(mpg)` grabs first/last row of data set
    - You can specify an argument `n` for the number of rows to grab
    - You can specify an argument `prop` for the proportion of rows to grab
+ `slice_sample(mpg, 10)` grabs 10 rows at random, without replacement
+ `slice_min(mpg, hwy)` / `slice_max(mpg, hwy)` gives the `n`/`prop` rows with the lowest/highest values of `hwy`.

---

## Exercise

1. Extract the rows from `penguins` with the smallest 10% of penguins by mass
1. Extract a random 20% of rows from `penguins`

```{r, echo=F}
countdown(minutes=5)
```


---
class: middle, center, inverse

# Workflow pipes in the tidyverse

---
  
## Pipes

Pipes are a method in R to create analytic pipelines utilizing tidyverse functions.

The pipe operator (denoted `%>%`, spoken as "then") is what creates the pipes.

You start with a dataset, and then progressively add functions to the pipe. Typically you save the result to a new object.

Each element of the pipe takes as its first argument the results of the previous step, which typically is a data frame.

Pipes are just a different representation of an analytic process that we can do in separate steps anyway. 

.footnote[The keyboard shortcut for the pipe operator in RStudio is .heatinline[`Ctrl/Cmd + Shift + m`]]

---

## Pipes

.pull-left[
Without pipes

```{r }
mpg1 <- mutate(mpg, id = 1:n())
mpg2 <- select(mpg1, id, year, trans, cty, hwy)
mpg_final <- mutate(mpg2, 
                    across(c(cty, hwy), 
                           function(x) {x * 1.6/3.8}))
```

]
.pull-right[
With pipes

```{r }
mpg_final <- mpg %>% 
  mutate(id = 1:n()) %>% 
  select(id, year, trans, cty, hwy) %>% 
  mutate(across(c(cty, hwy), 
                function(x){x*1.6/3.8}))
```

]

The important things to note here are:

1. When using pipes, the results of one operation are automatically entered into the **first argument** of the next function, so the actual specification omits the first argument
1. If you need the results of one step to go to some other argument of the next function, you can represent that input by `.`, for example, .fatinline[`mpg %>% lm(cty ~ hwy, data = .)`] takes the dataset `mpg` and places it in the argument for `data` in the `lm` function. 

---

## Exercise

<ol>
<li>Create a pipe that 
<ol type='a'>
  <li> starts with the _penguins_ dataset,
  <li> extracts the species, bill length and body mass columns, 
  <li> filters out rows with missing body mass measurements, and
  <li> creates a new columns with mass in pounds (1 lb = 453 grams) 
</ol>
</ol>
    
```{r, echo=F}
countdown(minutes=5)
```

---
class:middle, center, inverse

# Tidying data

---

## Tidy data

<div style="display:flex;align-items:center;font-size:30pt;font-family:'Roboto Slab';width:100%;height:300px;background-color:wheat;text-align:center;padding-left: 50px; padding-right: 0px;border: 1px solid red; position: relative;">

Tidy datasets are all alike, <br/>
but every messy data is messy in its own way

</div>

---

## Tidy data

Tidy data is a **computer-friendly** format based on the following characteristics:

- Each row is one observation
- Each column is one variable
- Each set of observational unit forms a table

All other forms of data can be considered **messy data**.

---

## Let us count the ways

There are many ways data can be messy. An incomplete list....

+ Column headers are values, not variables
+ Multiple variables are stored in a single column
+ Variables are stored in both rows and columns
+ Multiple types of observational units are saved in the same table
+ A single observational unit is stored in multiple tables

---

## Ways to have messy (i.e. not tidy) data

1. Column headers contain values

Country   |   < $10K    | $10-20K    | $20-50K   | $50-100K    | > $100K
----------|-------------|------------|-----------|-------------|---------
India     |   40        |  25        |   25      |  9          |  1
USA       |   20        |  20        |  20       | 30          |  10

---

## Ways to have messy (i.e. not tidy) data

Column headers contain values

Country   |   Income  | Percentage
----------|-----------|------------
India     |  < $10K   |  40
USA       |  < $10K   | 20

This is a case of reshaping or melting 

---

## Ways to have messy (i.e. not tidy) data

Multiple variables in one column

Country  | Year   | M_0-14  | F_0-14  | M_ 15-60  | F_15-60  | M_60+  | F_60+
---------|--------|---------|---------|-----------|----------|--------|-------
UK       |  2010  |         |         |           |          |        | 
UK       |  2011  |         |         |           |          |        | 

<p>
Separating columns into different variables

Country  | Year   | Gender  | Age    | Count
---------|--------|---------|--------|-------




---

## Tidying data

The typical steps are 

+ Transforming data from wide to tall (`pivot_longer`) and from tall to wide (`pivot_wider`)
+ Separating columns into different columns (`separate`)
+ Putting columns together into new variables (`unite`)

----
>The functions `pivot_longer` and `pivot_wider` supercede the older functions `gather` and `spread`, 
which I have used in previous iterations of this class. However, if you are familiar with `gather` and `spread`, they aren't gone and can still be used in the current **tidyr** package.

---

## Tidy data

```{r }
table1
```

Is this tidy?

---

## Tidy data

```{r }
table2
```

Is this tidy?

---

## Tidy data

```{r }
table3
```

Is this tidy?

---


## Tidy data

```{r }
table4a # cases
table4b # population
```

Are these tidy?

---

## Can we make datasets tidy?

Sometimes. The functions in the `tidyr` package can help

- `separate` is a function that can split a column into multiple columns
    - When there are multiple variables together in a column
    
```{r }
table3
```

We need to separate `rate` into two variables, cases and population

---



## Can we make datasets tidy?

```{r }
separate(table3, col = rate, into = c("cases", "population"), 
         sep = "/", 
         convert = TRUE) # convert type if possible 
```

> I've been explicit about naming all the options. R functions can work by 
position as well, so `separate(table3, rate, c('cases','population'), '/')` would work, but it's not very clear, is it?

---

## Can we make datasets tidy?

```{r }
table2
```

Here there are observations on two variables in successive rows

---

## Can we make datasets tidy?

We need to `spread` these rows out into different columns. This function is now called `pivot_wider`.

.pull-left[
![](../img/tidyr-spread-gather.gif)
]
.pull-right[
```{r }
pivot_wider(table2, names_from = type, values_from = count)
```
]

---

## Can we make datasets tidy?

```{r }
table4a
```

Here, the variable for year is stored as a header, not as data in a cell.

We need to `gather` that data and put it into a column. This function is now called `pivot_longer`

---

## Can we make datasets tidy?

.pull-left[
![](../img/tidyr-spread-gather.gif)
]

.pull-right[
```{r }
pivot_longer(table4a, names_to = 'year', values_to  = 'cases', 
    cols = c(`1999`, `2000`))
```
]

---

## Making data tidy

Admittedly, `pivot_wider` and `pivot_longer` are not easy concepts, but we'll practice with them more. 

1. `pivot_longer` collects multiple columns into 2, and only 2 columns
    - One column represents the data in the column headers
    - One column represents the values in the column
    - All other columns are repeated to keep all the data properly associated
1. `pivot_wider` takes two columns and makes them multiple columns
    - The values in one column form the headers to different new columns
    - The values in the other column represent the values in the corresponding cells
    - The other columns are repeated to start with, but reduce repetitions to make all associated data stay together
    



