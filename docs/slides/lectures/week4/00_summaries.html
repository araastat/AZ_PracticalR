<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Practical R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Abhijit Dasgupta" />
    <script src="00_summaries_files/header-attrs-2.6/header-attrs.js"></script>
    <link href="00_summaries_files/tachyons-4.12.0/tachyons.min.css" rel="stylesheet" />
    <link href="00_summaries_files/panelset-0.2.3.9000/panelset.css" rel="stylesheet" />
    <script src="00_summaries_files/panelset-0.2.3.9000/panelset.js"></script>
    <link href="00_summaries_files/xaringanExtra-extra-styles-0.2.3.9000/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="../css/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="../css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../css/sfah.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Practical R
### Abhijit Dasgupta
### 15 February, 2021

---












---



# Goals 

+ Learn how to join data sets (merging)

???

We need to 

+ put datasets capturing different attributes together to find a complete picture
+ evaluate different attributes to see if they contribute to our understanding
+ hedge our bets to ensure we find 

---
    
# Data

This data set is taken from a breast cancer proteome database available [here](https://www.kaggle.com/piotrgrabo/breastcancerproteomes) and modified for this exercise.

+ Clinical data: data/BreastCancer_Clinical.xlsx
+ Proteome data: data/BreastCancer_Expression.xlsx

&gt; These data are available in the class Canvas page and the expectation is that you will 
save them to the `data` folder of your project.

---
class: inverse, middle, center

# Joins

---
# Putting data sets together

+ Quite often, data on individuals lie in different tables

    - Clinical, demographic and bioinformatic data
--
    - Drug, procedure, and payment data (think Medicare)
--
    - Personal health data across different healthcare entities

---

# Joining data sets

The simplest case is when we just need to add more data to existing data 

- New patients in study, with same protocol (add rows)
- Adding pathology, imaging data for existing patients (add columns)

---

# Joining data sets

.pull-left[
&lt;span style="text-align:center;"&gt;`cbind(x,y)`&lt;/span&gt;
&lt;img src="/Users/abhijit/ARAASTAT/Teaching/AZ_PracticalR/slides/lectures/img/addcol.png" width="640" /&gt;

Data sets have same subjects/observations, but new variables
]
.pull-right[
&lt;span style="text-align:center;"&gt;`rbind(x,y)`&lt;/span&gt;
&lt;img src="/Users/abhijit/ARAASTAT/Teaching/AZ_PracticalR/slides/lectures/img/addrow.png" width="640" /&gt;

Data sets have same variables, but new subjects]

---

# Joining data sets

.pull-left[
We will talk about more general ways of joining two datasets

We will assume:

1. We have two rectangular data sets (so `data.frame` or `tibble`)
1. There is at least one variable (column) in common, even if they have different names
    - Patient ID number
    - SSN (Social Security number)
    - Identifiable information
]

.pull-right[

&lt;img src="../img/merge.png" height="10%"/&gt;
]

---

# Joining data sets

&lt;img width="100%" src="../img/joins.png"/&gt;

--

&lt;table width="100%"&gt;
&lt;tr&gt;
&lt;td style="text-align:center;"&gt;inner_join&lt;/td&gt;
&lt;td style="text-align:center;"&gt;left_join&lt;/td&gt;
&lt;td style="text-align:center;"&gt;right_join&lt;/td&gt;
&lt;td style="text-align:center;"&gt;outer_join&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

--

The "join condition" are the common variables in the two datasets, i.e. rows are selected if the values of the common variables in the left dataset matches the values of the common variables in the right dataset

These functions are available in the **dplyr** package.
---

## A breast cancer example




```r
library(readxl)
clinical &lt;- read_excel('data/BreastCancer_Clinical.xlsx', 
                       .name_repair = 'universal') # See ?tibble::tibble
proteome &lt;- read_excel('data/BreastCancer_Expression.xlsx', 
                       .name_repair = 'universal') 
```


.pull-left[

```r
clinical
```

```
# A tibble: 105 x 30
   Complete.TCGA.ID Gender Age.at.Initial.… ER.Status PR.Status HER2.Final.Stat…
   &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;           
 1 TCGA-A2-A0T2     FEMALE               66 Negative  Negative  Negative        
 2 TCGA-A2-A0CM     FEMALE               40 Negative  Negative  Negative        
 3 TCGA-BH-A18V     FEMALE               48 Negative  Negative  Negative        
 4 TCGA-BH-A18Q     FEMALE               56 Negative  Negative  Negative        
 5 TCGA-BH-A0E0     FEMALE               38 Negative  Negative  Negative        
 6 TCGA-A7-A0CE     FEMALE               57 Negative  Negative  Negative        
 7 TCGA-D8-A142     FEMALE               74 Negative  Negative  Negative        
 8 TCGA-A2-A0D0     FEMALE               60 Negative  Negative  Negative        
 9 TCGA-AO-A0J6     FEMALE               61 Negative  Negative  Negative        
10 TCGA-A2-A0YM     FEMALE               67 Negative  Negative  Negative        
# … with 95 more rows, and 24 more variables: Tumor &lt;chr&gt;,
#   Tumor..T1.Coded &lt;chr&gt;, Node &lt;chr&gt;, Node.Coded &lt;chr&gt;, Metastasis &lt;chr&gt;,
#   Metastasis.Coded &lt;chr&gt;, AJCC.Stage &lt;chr&gt;, Converted.Stage &lt;chr&gt;,
#   Survival.Data.Form &lt;chr&gt;, Vital.Status &lt;chr&gt;,
#   Days.to.Date.of.Last.Contact &lt;dbl&gt;, Days.to.date.of.Death &lt;dbl&gt;,
#   OS.event &lt;dbl&gt;, OS.Time &lt;dbl&gt;, PAM50.mRNA &lt;chr&gt;,
#   SigClust.Unsupervised.mRNA &lt;dbl&gt;, SigClust.Intrinsic.mRNA &lt;dbl&gt;,
#   miRNA.Clusters &lt;dbl&gt;, methylation.Clusters &lt;dbl&gt;, RPPA.Clusters &lt;chr&gt;,
#   CN.Clusters &lt;dbl&gt;, Integrated.Clusters..with.PAM50. &lt;dbl&gt;,
#   Integrated.Clusters..no.exp. &lt;dbl&gt;, Integrated.Clusters..unsup.exp. &lt;dbl&gt;
```
]
.pull-right[

```r
proteome
```

```
# A tibble: 83 x 11
   TCGA_ID NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780 NP_958783
   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
 1 TCGA-A…     1.10      1.11      1.11      1.11      1.12      1.11      1.11 
 2 TCGA-C…     2.61      2.65      2.65      2.65      2.65      2.65      2.65 
 3 TCGA-A…    -0.660    -0.649    -0.654    -0.632    -0.640    -0.654    -0.649
 4 TCGA-B…     0.195     0.215     0.215     0.205     0.215     0.215     0.215
 5 TCGA-C…    -0.494    -0.504    -0.501    -0.510    -0.504    -0.504    -0.501
 6 TCGA-C…     2.77      2.78      2.78      2.80      2.79      2.78      2.78 
 7 TCGA-E…     0.863     0.870     0.870     0.866     0.870     0.870     0.870
 8 TCGA-C…     1.41      1.41      1.41      1.41      1.41      1.41      1.41 
 9 TCGA-A…     1.19      1.19      1.19      1.19      1.20      1.19      1.19 
10 TCGA-A…     1.10      1.10      1.10      1.10      1.09      1.10      1.10 
# … with 73 more rows, and 3 more variables: NP_958784 &lt;dbl&gt;, NP_112598 &lt;dbl&gt;,
#   NP_001611 &lt;dbl&gt;
```

]

---

## A breast cancer example




```r
library(readxl)
clinical &lt;- read_excel('data/BreastCancer_Clinical.xlsx', 
                       .name_repair = 'universal') 
proteome &lt;- read_excel('data/BreastCancer_Expression.xlsx', 
                       .name_repair = 'universal') 
```
.pull-left[

```r
clinical[,1:2]
```

```
# A tibble: 105 x 2
   Complete.TCGA.ID Gender
   &lt;chr&gt;            &lt;chr&gt; 
 1 TCGA-A2-A0T2     FEMALE
 2 TCGA-A2-A0CM     FEMALE
 3 TCGA-BH-A18V     FEMALE
 4 TCGA-BH-A18Q     FEMALE
 5 TCGA-BH-A0E0     FEMALE
 6 TCGA-A7-A0CE     FEMALE
 7 TCGA-D8-A142     FEMALE
 8 TCGA-A2-A0D0     FEMALE
 9 TCGA-AO-A0J6     FEMALE
10 TCGA-A2-A0YM     FEMALE
# … with 95 more rows
```
]
.pull-right[

```r
proteome[,1:2]
```

```
# A tibble: 83 x 2
   TCGA_ID      NP_958782
   &lt;chr&gt;            &lt;dbl&gt;
 1 TCGA-AO-A12D     1.10 
 2 TCGA-C8-A131     2.61 
 3 TCGA-AO-A12B    -0.660
 4 TCGA-BH-A18Q     0.195
 5 TCGA-C8-A130    -0.494
 6 TCGA-C8-A138     2.77 
 7 TCGA-E2-A154     0.863
 8 TCGA-C8-A12L     1.41 
 9 TCGA-A2-A0EX     1.19 
10 TCGA-AO-A12D     1.10 
# … with 73 more rows
```
]

--

We see that both have the same ID variable, but with different names and different orders

???

Let's keep only the first two columns so we can see the ID variable

---

## A breast cancer example

Let's make sure that the ID's are truly IDs, i.e. each row has a unique value


```r
length(unique(clinical$Complete.TCGA.ID)) == nrow(clinical)
```

```
[1] TRUE
```
--


```r
length(unique(proteome$TCGA_ID)) == nrow(proteome)
```

```
[1] FALSE
```


--
&lt;div style="height:25%;margins:auto;"&gt;
&lt;img style="display:block; margin:0 auto; height: 50%;" src="https://twitchy.com/wp-content/uploads/2015/04/screen-shot-2015-04-13-at-2-06-38-pm-300x300.png"/&gt;
&lt;/div&gt;

???

We need the ID variables to be unique for each row. If we use multiple columns to define the "ID" then each row needs to have a unique set of values for those columns. Otherwise the joins get confused about 
which rows go with which rows. 

---
## Data example

For convenience we'll keep the first instance for each ID in the `proteome` data


```r
proteome &lt;- proteome %&gt;% filter(!duplicated(TCGA_ID))
```

&gt; `duplicated` = TRUE if a previous row contains the same value

--


```r
length(unique(proteome$TCGA_ID)) == nrow(proteome)
```

```
[1] TRUE
```

???

We don't have to sort data for duplicated

---

## Inner join

.pull-left[
![label](../img/inner-join.gif)
]
.pull-right[

+ Keep only rows that have common ids between the two data, and add columns
+ The joined data will have no more rows than either data, but more columns than each
]
---
## Inner join


```r
common_rows &lt;- inner_join(clinical[,1:6], proteome, 
                          by=c('Complete.TCGA.ID'='TCGA_ID'))
```

```
# A tibble: 77 x 16
   Complete.TCGA.ID Gender Age.at.Initial.… ER.Status PR.Status HER2.Final.Stat…
   &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;           
 1 TCGA-A2-A0CM     FEMALE               40 Negative  Negative  Negative        
 2 TCGA-BH-A18Q     FEMALE               56 Negative  Negative  Negative        
 3 TCGA-A7-A0CE     FEMALE               57 Negative  Negative  Negative        
 4 TCGA-D8-A142     FEMALE               74 Negative  Negative  Negative        
 5 TCGA-AO-A0J6     FEMALE               61 Negative  Negative  Negative        
 6 TCGA-A2-A0YM     FEMALE               67 Negative  Negative  Negative        
 7 TCGA-A2-A0D2     FEMALE               45 Negative  Negative  Negative        
 8 TCGA-A2-A0SX     FEMALE               48 Negative  Negative  Negative        
 9 TCGA-AO-A0JL     FEMALE               59 Negative  Negative  Negative        
10 TCGA-AO-A12F     FEMALE               36 Negative  Negative  Negative        
# … with 67 more rows, and 10 more variables: NP_958782 &lt;dbl&gt;, NP_958785 &lt;dbl&gt;,
#   NP_958786 &lt;dbl&gt;, NP_000436 &lt;dbl&gt;, NP_958781 &lt;dbl&gt;, NP_958780 &lt;dbl&gt;,
#   NP_958783 &lt;dbl&gt;, NP_958784 &lt;dbl&gt;, NP_112598 &lt;dbl&gt;, NP_001611 &lt;dbl&gt;
```

--

Note that we have all the columns from both datasets, but only the common set of IDs from the two datasets
--

&gt; Without the `by` option, R will attempt to join on all column names that are common between the data sets. 
If the ID columns have different names, you **must** use `by`. Even if they have the same names, it's 
good practice to be explicity

---

## Left join

.pull-left[
![label](../img/left-join.gif)
&lt;div style="font-size:12pt;"&gt;
All the animations are courtesy Garrick Aden-Buie and are available &lt;a src="https://github.com/gadenbuie/tidyexplain" target="_blank"&gt;here&lt;/a&gt;.
&lt;/div&gt;
]
.pull-right[
+ Keep all rows of left data, add columns from right data only for rows with matching IDs
+ If a row in left data has no corresponding row in the right data, the corresponding entries in the joined data are replaced by `NA`
+ Joined data has same number of rows as left data, but more columns.
]
---
## Left join

```r
left_rows &lt;- left_join(clinical[,1:6], proteome, by=c('Complete.TCGA.ID'='TCGA_ID'))
```

```
# A tibble: 105 x 16
  Complete.TCGA.ID Gender Age.at.Initial.Pathologic.Diagnosis ER.Status
  &lt;chr&gt;            &lt;chr&gt;                                &lt;dbl&gt; &lt;chr&gt;    
1 TCGA-A2-A0T2     FEMALE                                  66 Negative 
2 TCGA-A2-A0CM     FEMALE                                  40 Negative 
3 TCGA-BH-A18V     FEMALE                                  48 Negative 
  PR.Status HER2.Final.Status NP_958782 NP_958785 NP_958786 NP_000436 NP_958781
  &lt;chr&gt;     &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Negative  Negative             NA        NA        NA        NA        NA    
2 Negative  Negative              0.683     0.694     0.698     0.687     0.687
3 Negative  Negative             NA        NA        NA        NA        NA    
  NP_958780 NP_958783 NP_958784 NP_112598 NP_001611
      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1    NA        NA        NA         NA       NA    
2     0.698     0.698     0.698     -2.65    -0.984
3    NA        NA        NA         NA       NA    
# … with 102 more rows
```

We get 105 rows, which is all the rows of `clinical`, combined with the rows of `proteome` with common IDs. The rest of the rows get `NA` for the proteome columns.

---

## Right join

.pull-left[
![label](../img/right-join.gif)
]
.pull-right[
+ Keep all the rows of the _right_ data, add corresponding rows of left data _on the left_
+ Once again, if there are rows of right data that do not have corresponding rows in left data, the entries are filled with `NA`
+ The joined data has the same number of rows as the right data, but more columns (attached to its left). The order of the columns is the columns of the left data followed by the columns of the right data
]
---
## Right join

```r
right_rows &lt;- right_join(clinical[,1:6], proteome, by=c('Complete.TCGA.ID'='TCGA_ID'))
```

```
# A tibble: 80 x 16
  Complete.TCGA.ID Gender Age.at.Initial.Pathologic.Diagnosis ER.Status
  &lt;chr&gt;            &lt;chr&gt;                                &lt;dbl&gt; &lt;chr&gt;    
1 TCGA-A2-A0CM     FEMALE                                  40 Negative 
2 TCGA-BH-A18Q     FEMALE                                  56 Negative 
3 TCGA-A7-A0CE     FEMALE                                  57 Negative 
  PR.Status HER2.Final.Status NP_958782 NP_958785 NP_958786 NP_000436 NP_958781
  &lt;chr&gt;     &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Negative  Negative              0.683     0.694     0.698     0.687     0.687
2 Negative  Negative              0.195     0.215     0.215     0.205     0.215
3 Negative  Negative             -1.12     -1.12     -1.12     -1.13     -1.13 
  NP_958780 NP_958783 NP_958784 NP_112598 NP_001611
      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1     0.698     0.698     0.698     -2.65    -0.984
2     0.215     0.215     0.215     -1.04    -0.517
3    -1.12     -1.12     -1.12       2.24    -2.58 
# … with 77 more rows
```

--

Here we get 80 rows, which is all the rows of `proteome`, along with the rows of `clinical` with common IDs, but with the columns of `clinical` appearing first.

---
## Outer/Full Join

.pull-left[
![label](../img/full-join.gif)
]
.pull-right[
This is the _kitchen sink_ join

+ All rows of the left and right data are included
+ Non-corresponding entries are filled with `NA`
+ The joined data set has at least as many rows as the larger of the two data, and more columns than either data.

]
---
## Outer/Full Join


```r
full_rows &lt;- full_join(clinical[,1:6], proteome, by=c('Complete.TCGA.ID'='TCGA_ID'))
```

```
# A tibble: 108 x 16
  Complete.TCGA.ID Gender Age.at.Initial.Pathologic.Diagnosis ER.Status
  &lt;chr&gt;            &lt;chr&gt;                                &lt;dbl&gt; &lt;chr&gt;    
1 TCGA-A2-A0T2     FEMALE                                  66 Negative 
2 TCGA-A2-A0CM     FEMALE                                  40 Negative 
3 TCGA-BH-A18V     FEMALE                                  48 Negative 
  PR.Status HER2.Final.Status NP_958782 NP_958785 NP_958786 NP_000436 NP_958781
  &lt;chr&gt;     &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Negative  Negative             NA        NA        NA        NA        NA    
2 Negative  Negative              0.683     0.694     0.698     0.687     0.687
3 Negative  Negative             NA        NA        NA        NA        NA    
  NP_958780 NP_958783 NP_958784 NP_112598 NP_001611
      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1    NA        NA        NA         NA       NA    
2     0.698     0.698     0.698     -2.65    -0.984
3    NA        NA        NA         NA       NA    
# … with 105 more rows
```

--

Here we obtain 108 rows and 16 columns. So we've expanded the data in both rows and columns, putting missing values in where needed.

---
# Joins

In each of `inner_join`, `left_join`, `right_join` and `full_join`, the number of columns always increases 

There are also two joins where the number of columns don't increase. They aren't really "joins" in that sense, but really fancy filters on a dataset

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Join &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Use &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Description &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; semi_join &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; semi_join(A,B) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Keep rows in A where ID matches some ID value in B &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; anti_join &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; anti_join(A,B) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Keep rows in A where ID does NOT match any ID value in B &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

These just filter the rows of `A` without adding any columns of `B`. These can be faster than `dplyr::filter` when dealing with large data sets

---

# Putting it in a pipe


```r
final_data &lt;- clinical %&gt;% 
  inner_join(proteome, by=c("Complete.TCGA.ID"="TCGA_ID")) %&gt;% 
  filter(Gender =='FEMALE') %&gt;% 
  select(Complete.TCGA.ID, Age.at.Initial.Pathologic.Diagnosis, ER.Status,
         starts_with("NP")) # grabs all the protein data
```

```
# A tibble: 75 x 13
  Complete.TCGA.ID Age.at.Initial.Pathologic.Diagnosis ER.Status NP_958782
  &lt;chr&gt;                                          &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;
1 TCGA-A2-A0CM                                      40 Negative      0.683
2 TCGA-BH-A18Q                                      56 Negative      0.195
3 TCGA-A7-A0CE                                      57 Negative     -1.12 
  NP_958785 NP_958786 NP_000436 NP_958781 NP_958780 NP_958783 NP_958784
      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1     0.694     0.698     0.687     0.687     0.698     0.698     0.698
2     0.215     0.215     0.205     0.215     0.215     0.215     0.215
3    -1.12     -1.12     -1.13     -1.13     -1.12     -1.12     -1.12 
  NP_112598 NP_001611
      &lt;dbl&gt;     &lt;dbl&gt;
1     -2.65    -0.984
2     -1.04    -0.517
3      2.24    -2.58 
# … with 72 more rows
```

---

## Some notes

+ Joins are very much in the spirit of using SQL in databases
+ In SAS, if you use `MERGE` in the `DATA` step to create merged variables, you need to 
sort the data by the common variables
    - This is a very expensive operation computationally
    - In SAS, you can avoid this by using `PROC SQL`
    - In R, this sorting is not necessary
+ Learning to join data sets efficiently is one of the coolest skills of a data scientist, and 
makes life infinitely easier

---

## Example code: Joining many datasets together

**Requirement:** Pull together over 200 datasets of variant alleles and expressions (1 per subject/cell line) 


```r
library(dplyr)

fnames &lt;- dir('~/Desktop/Sreya', full.names = TRUE) # Grab and store the paths to the individual files
ids &lt;- stringr::str_extract(fnames, '[:alnum:]+') # The file names have the subject ids in them 
                                                  # as first bit of the string

## Data ingestion
data_corpus &lt;- purrr::map(fnames, read_delim, delim='\t') # Creates a list of raw datasets

## Data munging
for (i in 1:length(data_corpus)){
  data_corpus[[i]] &lt;- data_corpus[[i]] %&gt;% # Note [[]] since I'm manipulating lists
    select(`Variant Allele`, HF) %&gt;% # Keep only allele name and expression
    set_names("variant_allele", ids[i]) %&gt;% # change column names to `variant_allele` and subject ID
    mutate(variant_allele = str_trim(variant_allele)) # Getting rid of extra spaces
}

## Data joining
data_merged &lt;- Reduce(full_join, data_corpus) # Here is the join. This works since 
                                              # all the data sets have only `variant_allele` in common
```

&gt; We haven't seen two functions here: `purrr::map` and `Reduce`. I won't go into details here, but see the short version on next slide.
Also notice that the number of files to be joined is never specified in the code. This could work for any number
of files

---

## Example code: Joining many datasets together

+ The `map` function acts on a list (first argument) and applies a function (2nd argument) to each element, storing the result in a list the same size as the first argument. You could replace the map function with a for loop, but map is provably more efficient computationally. It is worth thinking about map like a for loop, though. [Nice tutorial](https://jennybc.github.io/purrr-tutorial/index.html)
+ `Reduce` is a very powerful function that is one of the functional programming functions in R, i.e., it is a function that acts on functions. It takes as inputs a function (in our case, `full_join`), and a list (in our case, data_corpus). The input function should take two arguments of the same type, as `full_join` does, and Reduce goes through the list, applying the function to the first two elements of the list, then to the result and the 3rd element, then to the result and 4th element,and so on.



---



class: middle,center,inverse

# Categorical variables

---

## What are categorical variables?

Categorical variables are variables that

+ have values defining categories of things
+ typically have a few unique values
+ may or may not be ordered
+ are not interval-scaled, i.e., their differences don't make sense _per se_



---

## What are categorical variables?

.pull-left[
#### Non-ordered

1. Race (White, Black, Hispanic, Asian, Native American)
1. Gender (Male, Female, Other)
1. Geographic regions (Africa, Asia, Europe, North America, South America)
1. Genes/Proteins
]
.pull-right[
#### Ordered

1. Income levels (&lt; $10K, $10K - $25K, $25K - $75K, $75K - $100K)
1. BMI categories (Underweight, Normal, Overweight, Obese)
1. Number of bedrooms in houses (1 BR, 2BR, 3BR, 4BR)
]

---
class: middle, center

# Cateogical variables in R

---

## The `factor` data type

R stores categorical variables as type `factor`.

+ You can coerce a character or numeric object into a factor using `as.factor`.
+ You can check if an object is a factor with `is.factor`.
+ You can create a factor with the function `factor`.


---

## The `factor` data type

&gt; `factor(x = character(), levels, labels = levels,
       exclude = NA, ordered = is.ordered(x), nmax = NA)`
&gt;
&gt; factor returns an object of class "factor" which has a set of integer codes the length of x with a "levels" attribute of mode character and unique

----

+ Internally, each level of a factor is coded as an integer
+ Each such integer has a corresponding `level` which is a character, describing the level. 
+ You can add `labels` to each level to change the printed form of the factor. 


---

## The `factor` data type


```r
x &lt;- c('Maryland','Virginia', 'District', 'Maryland','Virginia')  # a character vector
xf &lt;- as.factor(x)
xf
```

```
[1] Maryland Virginia District Maryland Virginia
Levels: District Maryland Virginia
```

There are three levels, that by default are in alphabetical order

----

.pull-left[

```r
as.integer(xf)
```

```
[1] 2 3 1 2 3
```

+ District = 1, Maryland = 2, Virginia = 3
]
.pull-right[

```r
as.character(xf)
```

```
[1] "Maryland" "Virginia" "District" "Maryland" "Virginia"
```

+ Get original characters back
]



---

## The `factor` data type


```r
y &lt;- c(5, 3, 9, 4, 5, 3)
yf &lt;- as.factor(y)
yf
```

```
[1] 5 3 9 4 5 3
Levels: 3 4 5 9
```

Levels are still in alphanumeric order

----

.pull-left[

```r
as.numeric(yf)
```

```
[1] 3 1 4 2 3 1
```

+ Note, we don't get original integers back!!
+ 3 = 1, 4 = 2, 5 = 3, 9 = 4
]
.pull-right[

```r
as.numeric(as.character(yf))
```

```
[1] 5 3 9 4 5 3
```

+ This is how you get numbers back
]

---

## The `factor` data type


.pull-left[

```r
x &lt;- c('MD','DC','VA','MD','DC')
xf &lt;- factor(x)
unclass(xf)
```

```
[1] 2 1 3 2 1
attr(,"levels")
[1] "DC" "MD" "VA"
```

]
.pull-right[

```r
x &lt;- c('MD','DC','VA','MD','DC')
xf &lt;- factor(x, levels = c('MD','DC','VA'))
unclass(xf)
```

```
[1] 1 2 3 1 2
attr(,"levels")
[1] "MD" "DC" "VA"
```

]

----

+ If I change the level designation, the underlying coding changes
+ This is important when a factor is an independent variable in a regression model

---

## The `factor` data type

The `drv` variable in the `mpg` dataset tells us the kind of drive (front, rear or 4-wheel) each car has. However
it's coded as `f`, `r`, and `4`, which is not great for display purposes. We can re-label these levels, but we have to be a bit careful

.pull-left[

```r
x &lt;- mpg$drv
xf &lt;- factor(x, 
              levels = c('4-wheel','Front wheel',
                         'Rear wheel'))
head(xf)
```

```
[1] &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;
Levels: 4-wheel Front wheel Rear wheel
```

]
.pull-right[

```r
x &lt;- mpg$drv
xf &lt;- factor(x, 
*             levels = c('4', 'f','r'),
*             labels = c('4-wheel','Front wheel',
*                        'Rear wheel'))
head(xf)
```

```
[1] Front wheel Front wheel Front wheel Front wheel Front wheel Front wheel
Levels: 4-wheel Front wheel Rear wheel
```

]

----

Levels have to match what's actually in the original data, but you can re-label the levels.

---
class: middle, center

# Why factors?

---

## Factors are R's discrete data type

+ Factors are interpreted as discrete by R's functions

.pull-left[

```r
ggplot(mpg,
       aes(year, cty))+
  geom_boxplot()
```

```
Warning: Continuous x aesthetic -- did you forget aes(group=...)?
```

![](00_summaries_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;
]
.pull-right[

```r
ggplot(mpg, 
       aes(as.factor(year), cty))+
  geom_boxplot()
```

![](00_summaries_files/figure-html/unnamed-chunk-47-1.png)&lt;!-- --&gt;
]

---

## Dummy variables are automatically created from factors

.pull-left[

```r
model.matrix(~species, data = palmerpenguins::penguins)
```

```
  (Intercept) speciesChinstrap speciesGentoo
1           1                0             0
2           1                0             0
3           1                1             0
4           1                1             0
5           1                0             1
6           1                0             1
attr(,"assign")
[1] 0 1 1
attr(,"contrasts")
attr(,"contrasts")$species
[1] "contr.treatment"
```
]
.pull-right[
+ If a factor has _n_ levels, you get _n-1_ dummy variables
+ The level corresponding to integer code 1 is omitted as the reference level
]

----

Changing the base level (integer code 1) changes model interpretation since
it changes the reference level against which all other levels are compared.

---
class: middle, center

# Manipulating factors&lt;br/&gt;&lt;br/&gt;The `forcats` package (part of `tidyverse`)

---

## Effect in models

.pull-left[

```r
library(palmerpenguins)
m &lt;- lm(body_mass_g ~ species, data = penguins)
broom::tidy(m)
```

```
# A tibble: 3 x 5
  term             estimate std.error statistic   p.value
  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)        3701.       37.6    98.4   2.49e-251
2 speciesChinstrap     32.4      67.5     0.480 6.31e-  1
3 speciesGentoo      1375.       56.1    24.5   5.42e- 77
```
Compare with Adele
]
.pull-right[

```r
p1 &lt;- penguins %&gt;% 
  mutate(species = fct_relevel(species, 'Gentoo'))
m1 &lt;- lm(body_mass_g ~ species, data=p1 )
broom::tidy(m1)
```

```
# A tibble: 3 x 5
  term             estimate std.error statistic   p.value
  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)         5076.      41.7     122.  6.86e-282
2 speciesAdelie      -1375.      56.1     -24.5 5.42e- 77
3 speciesChinstrap   -1343.      69.9     -19.2 3.21e- 56
```
Compare with Gentoo
]

----

Providing only one level to `fct_relevel` makes that the base level (integer code 1).  
You can also fully specify all the levels in order, or partially specify them. If you 
partially specify them, the remaining levels will be put in alphabetical order after the
ones you specify.

---

## Effect in plots

.pull-left[

```r
ggplot(penguins, 
       aes(x = species))+
  geom_bar()
```

![](00_summaries_files/figure-html/unnamed-chunk-52-1.png)&lt;!-- --&gt;

]
.pull-right[

```r
ggplot(p1, 
       aes(x = species))+
  geom_bar()
```

![](00_summaries_files/figure-html/unnamed-chunk-53-1.png)&lt;!-- --&gt;

]

Changes the order in which bars are plotted

---

## Extra levels

.pull-left[

```r
x &lt;- factor(str_split('statistics', '')[[1]], 
            levels = letters)
x
```

```
 [1] s t a t i s t i c s
Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z
```

```r
p1 &lt;- penguins %&gt;% filter(species != 'Gentoo')
fct_count(p1$species)
```

```
# A tibble: 3 x 2
  f             n
  &lt;fct&gt;     &lt;int&gt;
1 Adelie      152
2 Chinstrap    68
3 Gentoo        0
```

]
.pull-right[

```r
fct_drop(x)
```

```
 [1] s t a t i s t i c s
Levels: a c i s t
```

```r
p1 &lt;- p1 %&gt;% mutate(species = fct_drop(species))
fct_count(p1$species)
```

```
# A tibble: 2 x 2
  f             n
  &lt;fct&gt;     &lt;int&gt;
1 Adelie      152
2 Chinstrap    68
```

]

Getting rid of extra levels&lt;br/&gt;&lt;br/&gt;
Sometimes levels with no data show up in summaries or plots

---

# Ordering levels by frequency

.pull-left[

```r
ggplot(penguins, 
       aes(x = species))+
  geom_bar()
```

![](00_summaries_files/figure-html/unnamed-chunk-58-1.png)&lt;!-- --&gt;

]
.pull-right[

```r
ggplot(penguins,
       aes(x = fct_infreq(species)))+
  geom_bar()
```

![](00_summaries_files/figure-html/unnamed-chunk-59-1.png)&lt;!-- --&gt;

]

Ordering levels from most to least frequent

---

## Ordering levels by values of another variable

.pull-left[

```r
ggplot(penguins, 
       aes(x = species,
           y = bill_length_mm))+
  geom_boxplot()
```

![](00_summaries_files/figure-html/unnamed-chunk-60-1.png)&lt;!-- --&gt;
]
.pull-right[

```r
ggplot(penguins, 
       aes(x = fct_reorder(species, bill_length_mm, 
                           .fun=median, na.rm=T),
           y = bill_length_mm))+
  geom_boxplot() + labs(x = 'species')
```

![](00_summaries_files/figure-html/unnamed-chunk-61-1.png)&lt;!-- --&gt;

]

`fct_reorder` is useful for ordering a plot by ascending or descending levels. This 
makes the plot easier to read. 

---

## Ordering levels by values of another variable





```r
USArrests &lt;- USArrests %&gt;% rownames_to_column('State')
```



.pull-left[

```r
ggplot(USArrests, aes(x=State, y = Murder))+
  geom_bar(stat = 'identity') + 
  theme(axis.text = element_text(size=6))+
  coord_flip()
```

![](00_summaries_files/figure-html/unnamed-chunk-64-1.png)&lt;!-- --&gt;

]
.pull-right[

```r
ggplot(USArrests, aes(
  x = fct_reorder(State, Murder),
  y = Murder))+
  geom_bar(stat = 'identity')+
  theme(axis.text = element_text(size=6))+
  coord_flip()
```

![](00_summaries_files/figure-html/unnamed-chunk-65-1.png)&lt;!-- --&gt;

]

---

## Order levels based on last values when plotting 2 variables

The level ordering also shows up in the order of levels in the legends of plots. 
Suppose you are plotting two variables, grouped by a factor.

.pull-left[

```r
ggplot(iris, aes(
  x = Sepal.Length, 
  y = Sepal.Width, 
  color = Species))+
  geom_smooth(se=F)
```

![](00_summaries_files/figure-html/unnamed-chunk-66-1.png)&lt;!-- --&gt;

]
.pull-right[

```r
ggplot(iris, aes(
  x = Sepal.Length, 
  y = Sepal.Width, 
  color = fct_reorder2(Species,
                       Sepal.Length, Sepal.Width)))+
  geom_smooth(se=F) + labs(color = 'Species')
```

![](00_summaries_files/figure-html/unnamed-chunk-67-1.png)&lt;!-- --&gt;

]

---

## Further exploration

1. [forcats cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/factors.pdf)
1. [Chapter 15](https://r4ds.had.co.nz/factors.html) of R4DS





---

## For loops

For-loops are a computational structure that allows you to do the same thing
repeatedly over a loop with some index. 

The basic structure is


```r
for (variable in vector) {
  &lt;code to execute for each iteration&gt;
}
```

---

## Lists

Directly using lists has efficiency advantages. `rio` can load all the datasets
into a list, for example.


```r
sites &lt;- c('Brain','Colon','Esophagus','Lung','Oral')
dats &lt;- rio::import_list(file.path(paste0(sites,'.csv')))
names(dats)
```

```
[1] "Brain"     "Colon"     "Esophagus" "Lung"      "Oral"     
```

```r
str(dats[['Brain']])
```

```
'data.frame':	43 obs. of  10 variables:
 $ Year of Diagnosis   : chr  "1975-2016" "1975" "1976" "1977" ...
 $ All Races,Both Sexes: num  6.59 5.85 5.82 6.17 5.76 6.12 6.3 6.51 6.42 6.31 ...
 $ All Races,Males     : num  7.88 6.84 7.14 7.76 6.79 7.42 7.58 8.07 7.93 7.6 ...
 $ All Races,Females   : num  5.51 5.01 4.68 4.89 4.91 5.01 5.24 5.2 5.24 5.19 ...
 $ Whites,Both Sexes   : num  7.22 6.21 6.18 6.6 6.1 6.6 6.81 6.9 6.92 6.88 ...
 $ Whites,Males        : num  8.61 7.31 7.51 8.26 7.19 8.03 8.2 8.44 8.57 8.2 ...
 $ Whites,Females      : num  6.04 5.28 5.03 5.27 5.19 5.37 5.65 5.63 5.64 5.74 ...
 $ Blacks,Both Sexes   : num  4.08 4.14 3.32 3.55 3.86 3.69 3.14 5.02 3.71 2.75 ...
 $ Blacks,Males        : num  4.79 4.31 5.37 5.17 4.34 4.19 3.35 7.24 4.4 3.79 ...
 $ Blacks,Females      : chr  "3.51" "3.88" "-" "2.47" ...
```

---

## A note on `rio::import` for reading CSV files

The function `rio::import` reads CSV files using `data.table::fread`, and then converts the resulting `data.table` object into a `data.frame` object.

`fread` is not only really fast, but also makes some great automatic choices.

- It looks for and tries to omit non-standard header rows (so we don't need `skip=4`)
- It automatically tries to figure out the right number of rows to import
- With the `check.names=TRUE` option, it fixes issues with column names to make them conformant with R

--

.bg-washed-green.b--green.ba.bw2.br3.shadow-5.ph4.mt5[
Using `rio::import` solves a lot of troublesome things in importing regular text files (CSV, TSV, etc), and is **recommended**
]

---

## Lists


```r
dats &lt;- rio::import_list(file.path('../data', paste0(sites,'.csv')),
                         setclass = 'tbl',     # Output as tibble
                         check.names = TRUE)   # Check and fix names
str(dats[['Brain']])
```

```
tibble [43 × 10] (S3: tbl_df/tbl/data.frame)
 $ Year.of.Diagnosis   : chr [1:43] "1975-2016" "1975" "1976" "1977" ...
 $ All.Races.Both.Sexes: num [1:43] 6.59 5.85 5.82 6.17 5.76 6.12 6.3 6.51 6.42 6.31 ...
 $ All.Races.Males     : num [1:43] 7.88 6.84 7.14 7.76 6.79 7.42 7.58 8.07 7.93 7.6 ...
 $ All.Races.Females   : num [1:43] 5.51 5.01 4.68 4.89 4.91 5.01 5.24 5.2 5.24 5.19 ...
 $ Whites.Both.Sexes   : num [1:43] 7.22 6.21 6.18 6.6 6.1 6.6 6.81 6.9 6.92 6.88 ...
 $ Whites.Males        : num [1:43] 8.61 7.31 7.51 8.26 7.19 8.03 8.2 8.44 8.57 8.2 ...
 $ Whites.Females      : num [1:43] 6.04 5.28 5.03 5.27 5.19 5.37 5.65 5.63 5.64 5.74 ...
 $ Blacks.Both.Sexes   : num [1:43] 4.08 4.14 3.32 3.55 3.86 3.69 3.14 5.02 3.71 2.75 ...
 $ Blacks.Males        : num [1:43] 4.79 4.31 5.37 5.17 4.34 4.19 3.35 7.24 4.4 3.79 ...
 $ Blacks.Females      : chr [1:43] "3.51" "3.88" "-" "2.47" ...
```

---
class: center, middle, inverse

# purrr::map

---

## Map

`map` is like a for-loop, but strictly for lists. It is more efficient than for-loops. The basic templates are:


```r
map(&lt;list&gt;, &lt;function&gt;, &lt;function arguments&gt;)
map(&lt;list&gt;, &lt;function&gt;(&lt;arguments&gt;){&lt;definiition&gt;})
map(&lt;list&gt;, ~ &lt;definition with .x placeholder&gt;)
```

with the first argument of the function being the entry point for each component
of the list (or replacing the `.x` placeholder)


For example, if we want to take out the first row of each dataset and make sure all 
the variables are numeric, we could do:


```r
library(tidyverse)
dats &lt;- map(dats, 
            function(d){
              d %&gt;% dplyr::slice(-1) %&gt;%  # remove first row which contains summary data
                mutate(across(where(is.character), as.numeric))
            })
str(dats[['Brain']])
```

```
tibble [42 × 10] (S3: tbl_df/tbl/data.frame)
 $ Year.of.Diagnosis   : num [1:42] 1975 1976 1977 1978 1979 ...
 $ All.Races.Both.Sexes: num [1:42] 5.85 5.82 6.17 5.76 6.12 6.3 6.51 6.42 6.31 6.12 ...
 $ All.Races.Males     : num [1:42] 6.84 7.14 7.76 6.79 7.42 7.58 8.07 7.93 7.6 7.18 ...
 $ All.Races.Females   : num [1:42] 5.01 4.68 4.89 4.91 5.01 5.24 5.2 5.24 5.19 5.2 ...
 $ Whites.Both.Sexes   : num [1:42] 6.21 6.18 6.6 6.1 6.6 6.81 6.9 6.92 6.88 6.49 ...
 $ Whites.Males        : num [1:42] 7.31 7.51 8.26 7.19 8.03 8.2 8.44 8.57 8.2 7.64 ...
 $ Whites.Females      : num [1:42] 5.28 5.03 5.27 5.19 5.37 5.65 5.63 5.64 5.74 5.49 ...
 $ Blacks.Both.Sexes   : num [1:42] 4.14 3.32 3.55 3.86 3.69 3.14 5.02 3.71 2.75 4.53 ...
 $ Blacks.Males        : num [1:42] 4.31 5.37 5.17 4.34 4.19 3.35 7.24 4.4 3.79 5.34 ...
 $ Blacks.Females      : num [1:42] 3.88 NA 2.47 3.51 3.23 2.92 3.16 3.05 1.84 3.88 ...
```

The argument for the function inside the `map` function is an element of the list. In this case, it is a data frame.

The output of `map` is a list the same length as the input list.

Here, we are overwriting the original data. This is fine if you're sure about the transformations, but normally you might want to save the result with a different name first until you're sure of what you're doing

---


I don't like the names with dots, say 😄. I can just apply a function to each data set to fix that.


```r
dats &lt;- map(dats, janitor::clean_names) # assumes first argument gets elements of dats
str(dats[['Oral']])
```

```
tibble [42 × 10] (S3: tbl_df/tbl/data.frame)
 $ year_of_diagnosis   : num [1:42] 1975 1976 1977 1978 1979 ...
 $ all_races_both_sexes: num [1:42] 13.2 13.3 12.7 13.4 14 ...
 $ all_races_males     : num [1:42] 21.2 21 20.1 20.9 21.9 ...
 $ all_races_females   : num [1:42] 7.09 7.39 6.94 7.71 7.98 7.91 7.91 7.93 7.24 7.86 ...
 $ whites_both_sexes   : num [1:42] 13.3 13.2 12.6 13.2 13.7 ...
 $ whites_males        : num [1:42] 21.7 21.1 19.9 20.7 21.6 ...
 $ whites_females      : num [1:42] 6.94 7.38 7 7.57 7.72 7.62 7.95 7.85 7.28 7.64 ...
 $ blacks_both_sexes   : num [1:42] 13.4 15.2 14.5 15.9 18.5 ...
 $ blacks_males        : num [1:42] 20.2 23.8 23.9 26 28.2 ...
 $ blacks_females      : num [1:42] 8.23 8.37 6.77 8.18 10.77 ...
```

.bg-washed-green.b--green.ba.bw2.br3.ph4.mt1[
Note that `janitor::clean_names` takes a data.frame/tibble as its first argument (as all tidyverse functions), and `dats` is a list of tibbles. So `map` applies the `clean_names` function to each tibble in the list, and returns the result as a list
]
---


Now let's split up by sexes


```r
dats_all &lt;- map(dats, select, year_of_diagnosis, ends_with('sexes'))
dats_male &lt;- map(dats, select, year_of_diagnosis, ends_with('_males'))
dats_female &lt;- map(dats, select, year_of_diagnosis, ends_with('females'))
str(dats_all[['Esophagus']])
```

```
tibble [42 × 4] (S3: tbl_df/tbl/data.frame)
 $ year_of_diagnosis   : num [1:42] 1975 1976 1977 1978 1979 ...
 $ all_races_both_sexes: num [1:42] 4.14 4.3 4.06 4.12 4.42 4.27 4.14 4.26 4.29 4.18 ...
 $ whites_both_sexes   : num [1:42] 3.55 3.72 3.33 3.41 3.73 3.54 3.31 3.46 3.57 3.52 ...
 $ blacks_both_sexes   : num [1:42] 10.9 10.7 12 13.1 12.9 ...
```

--

.bg-washed-green.b--green.ba.bw2.br3.ph4.mt2[
Here I used the form `map(&lt;list&gt;, &lt;function&gt;, &lt;function arguments&gt;)`.

Earlier I had used `map(&lt;list&gt;,&lt;function definition&gt;)` and `map(&lt;list&gt;, &lt;function&gt;)` with no (i.e., default) arguments.

Note, `map` assumes that each element of the list is the **first** argument of the function, and so you only have to specify from the 2nd argument onwards
]

---


Let's make the column headers of each dataset reflect the site, so that when we join we can keep the 
sites separate


```r
for(n in sites){
  names(dats_all[[n]]) &lt;- str_replace(names(dats_all[[n]]), 'both_sexes',n)
  names(dats_male[[n]]) &lt;- str_replace(names(dats_male[[n]]), 'male',n)
  names(dats_female[[n]]) &lt;- str_replace(names(dats_female[[n]]), 'female',n)
}
names(dats_all[['Esophagus']])
```

```
[1] "year_of_diagnosis"   "all_races_Esophagus" "whites_Esophagus"   
[4] "blacks_Esophagus"   
```

---

.pull-left[
When we joined these data sets, we had to repeatedly use `left_join` to create the final data set. 


```r
joined_all &lt;- dats_all[['Brain']]
for(n in setdiff(names(dats2_all), 'Brain')){
  joined_all &lt;- joined %&gt;% left_join(dats_all[['n']])
}
```
]
.pull-right[
There is a shortcut to this repeated operation of a function with two inputs as applied to a list successively.


```r
joined_all &lt;- Reduce(left_join, dats_all)
joined_male &lt;- Reduce(left_join, dats_male)
joined_female &lt;- Reduce(left_join, dats_female)
```
]

--

```
tibble [42 × 16] (S3: tbl_df/tbl/data.frame)
 $ year_of_diagnosis  : num [1:42] 1975 1976 1977 1978 1979 ...
 $ all_races_Brain    : num [1:42] 5.85 5.82 6.17 5.76 6.12 6.3 6.51 6.42 6.31 6.12 ...
 $ whites_Brain       : num [1:42] 6.21 6.18 6.6 6.1 6.6 6.81 6.9 6.92 6.88 6.49 ...
 $ blacks_Brain       : num [1:42] 4.14 3.32 3.55 3.86 3.69 3.14 5.02 3.71 2.75 4.53 ...
 $ all_races_Colon    : num [1:42] 59.5 61.3 62.4 62 62.4 ...
 $ whites_Colon       : num [1:42] 60.2 62.2 63.2 62.8 63 ...
 $ blacks_Colon       : num [1:42] 56.9 55 60.8 62.2 58.6 ...
 $ all_races_Esophagus: num [1:42] 4.14 4.3 4.06 4.12 4.42 4.27 4.14 4.26 4.29 4.18 ...
 $ whites_Esophagus   : num [1:42] 3.55 3.72 3.33 3.41 3.73 3.54 3.31 3.46 3.57 3.52 ...
 $ blacks_Esophagus   : num [1:42] 10.9 10.7 12 13.1 12.9 ...
 $ all_races_Lung     : num [1:42] 52.2 55.4 56.7 57.8 58.6 ...
 $ whites_Lung        : num [1:42] 51.9 54.6 55.9 57.2 58 ...
 $ blacks_Lung        : num [1:42] 64.5 72.3 73.6 74.4 74.5 ...
 $ all_races_Oral     : num [1:42] 13.2 13.3 12.7 13.4 14 ...
 $ whites_Oral        : num [1:42] 13.3 13.2 12.6 13.2 13.7 ...
 $ blacks_Oral        : num [1:42] 13.4 15.2 14.5 15.9 18.5 ...
```

---

Next, we want to separate the races from the sites, after a `pivot_longer`. The `all_races` will pose a problem if we split on `_`. Let's fix that.


```r
names(joined_all) &lt;- str_replace(names(joined_all), 'all_races','allraces')
names(joined_male) &lt;- str_replace(names(joined_male), 'all_races','allraces')
names(joined_female) &lt;- str_replace(names(joined_female), 'all_races','allraces')
```

---

Now, for each of these, we need to gather then separate. We'll put the data sets in a list first


```r
joined &lt;- list('both'=joined_all, 'male'=joined_male, 'female'=joined_female)
joined &lt;- map(joined,
             function(d){
               d %&gt;% 
                pivot_longer(names_to='variable', values_to = 'rate',
                             cols=c(-year_of_diagnosis)) %&gt;% 
                 separate(variable, c('race','site'), sep='_')
             })
str(joined[['both']])
```

```
tibble [630 × 4] (S3: tbl_df/tbl/data.frame)
 $ year_of_diagnosis: num [1:630] 1975 1975 1975 1975 1975 ...
 $ race             : chr [1:630] "allraces" "whites" "blacks" "allraces" ...
 $ site             : chr [1:630] "Brain" "Brain" "Brain" "Colon" ...
 $ rate             : num [1:630] 5.85 6.21 4.14 59.54 60.2 ...
```

.bg-lightest-blue.b--blue.ba.bw2.br3.ph4.mt1[
Okay, this is voodoo 👿 Not really. Grab one of the datasets
and work out what you need. Since you'll be doing the same to all of the datasets, you use `map` on the list of datasets
]
---

## Final graphing

Now we're in a position to do the graphing. 

.left-column70[

```r
pltlist &lt;- joined[['both']] %&gt;% group_split(race) %&gt;% 
  map(function(d) {ggplot(d, 
                          aes(x = year_of_diagnosis,
                              y = rate,
                              color=site))+
  geom_point(show.legend = F) })
cowplot::plot_grid(plotlist=pltlist, ncol=1, 
                   labels=c('All','Whites','Blacks'))
```

I'm using quite advanced R here, but hopefully you'll learn by example. 

`group_split` splits the dataset by the values of the grouping variable into a list

]
.right-column70[
![](00_summaries_files/figure-html/07-Maps-Table1-16-1.png)&lt;!-- --&gt;
]





---
class: middle,center,inverse

# Statistical summaries


---

## Where we're going

1. Creating data summaries
1. Basic statistical comparisons between groups
1. Creating tables 
    -  Table 1
    -  Tables for analytic results

--

The basic assumption we'll make is that we will start with a tidy data set.


---
class: middle, center

# Statistical summaries

---

## Univariate summaries

**Single summaries**  
.pull-left[
- Mean (`mean`)
- Variance(`var`)
- Standard deviation (`sd`)
- Count (`nrow` or `dplyr::n` or `dplyr::n_distinct`)
]
.pull-right[
- Median (`median`)
- Inter-quartile range (`IQR`)
- Mean absolute deviation (`mad`)
- Minimum (`min`) and Maximum (`max`)
]

--

**Multiple summaries**  
- Quantiles (`quantile`)
- Range (`range`)

---
class: middle, center

# Summarizing the breast cancer expression dataset

---

## Mean


```r
brca &lt;- rio::import('../../data/BreastCancer_Expression.csv')
brca %&gt;% 
  summarize(across(starts_with('NP'), 
*              mean, na.rm=T))
```

```
  NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780 NP_958783
1 0.3202321 0.3269153 0.3264254 0.3236833 0.3270832 0.3263382 0.3259212
  NP_958784  NP_112598 NP_001611
1 0.3259995 -0.3074577 0.4578748
```

---

## Median


```r
brca %&gt;% 
  summarize(across(starts_with('NP'), 
*              median, na.rm=T))
```

```
  NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780 NP_958783
1 0.3236627 0.3269726 0.3269726 0.3302826 0.3269726 0.3269726 0.3269726
  NP_958784  NP_112598 NP_001611
1 0.3269726 -0.6021319 0.6948104
```

---

## Standard deviation


```r
brca %&gt;% 
  summarize(across(starts_with('NP'),
*              sd, na.rm=T))
```

```
  NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780 NP_958783
1 0.9767777 0.9800721 0.9799358 0.9784656 0.9806001 0.9796277 0.9806739
  NP_958784 NP_112598 NP_001611
1 0.9807512  2.024663  1.496951
```

---

## Multiple summaries together


```r
brca %&gt;% 
  summarize(across(starts_with('NP'),
               c(mean,
                 median,
                 sd), na.rm=T))
```

```
  NP_958782_1 NP_958782_2 NP_958782_3 NP_958785_1 NP_958785_2 NP_958785_3
1   0.3202321   0.3236627   0.9767777   0.3269153   0.3269726   0.9800721
  NP_958786_1 NP_958786_2 NP_958786_3 NP_000436_1 NP_000436_2 NP_000436_3
1   0.3264254   0.3269726   0.9799358   0.3236833   0.3302826   0.9784656
  NP_958781_1 NP_958781_2 NP_958781_3 NP_958780_1 NP_958780_2 NP_958780_3
1   0.3270832   0.3269726   0.9806001   0.3263382   0.3269726   0.9796277
  NP_958783_1 NP_958783_2 NP_958783_3 NP_958784_1 NP_958784_2 NP_958784_3
1   0.3259212   0.3269726   0.9806739   0.3259995   0.3269726   0.9807512
  NP_112598_1 NP_112598_2 NP_112598_3 NP_001611_1 NP_001611_2 NP_001611_3
1  -0.3074577  -0.6021319    2.024663   0.4578748   0.6948104    1.496951
```

---

## Multiple summaries together


```r
brca %&gt;% 
  summarize(across(-1, # got tired of typing
               c('Mean'=mean,
                 'Median' = median,
                 'SD'=sd), na.rm=T)) 
```

```
  NP_958782_Mean NP_958782_Median NP_958782_SD NP_958785_Mean NP_958785_Median
1      0.3202321        0.3236627    0.9767777      0.3269153        0.3269726
  NP_958785_SD NP_958786_Mean NP_958786_Median NP_958786_SD NP_000436_Mean
1    0.9800721      0.3264254        0.3269726    0.9799358      0.3236833
  NP_000436_Median NP_000436_SD NP_958781_Mean NP_958781_Median NP_958781_SD
1        0.3302826    0.9784656      0.3270832        0.3269726    0.9806001
  NP_958780_Mean NP_958780_Median NP_958780_SD NP_958783_Mean NP_958783_Median
1      0.3263382        0.3269726    0.9796277      0.3259212        0.3269726
  NP_958783_SD NP_958784_Mean NP_958784_Median NP_958784_SD NP_112598_Mean
1    0.9806739      0.3259995        0.3269726    0.9807512     -0.3074577
  NP_112598_Median NP_112598_SD NP_001611_Mean NP_001611_Median NP_001611_SD
1       -0.6021319     2.024663      0.4578748        0.6948104     1.496951
```

---

## Multiple summaries together

.left-column70[

```r
brca %&gt;% 
  summarize(across(-1, 
               c('Mean' = mean,
                 'Median' = median,
                 'SD' = sd), na.rm=T)) %&gt;% 
  pivot_longer(cols=everything(),
               names_to='variable', 
               values_to='value') %&gt;% 
  # extract(variable, c('ID','Statistic'), 
          # regex = '(NP_\\d+)_([A-Za-z]+)') %&gt;% 
* separate(variable,
*         c("Type",'ID','Statistic'), sep='_') %&gt;%
* pivot_wider(names_from = Statistic, values_from = value) %&gt;%
* unite(ID, c('Type','ID'), sep='_')
```

```
# A tibble: 10 x 4
   ID          Mean Median    SD
   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
 1 NP_958782  0.320  0.324 0.977
 2 NP_958785  0.327  0.327 0.980
 3 NP_958786  0.326  0.327 0.980
 4 NP_000436  0.324  0.330 0.978
 5 NP_958781  0.327  0.327 0.981
 6 NP_958780  0.326  0.327 0.980
 7 NP_958783  0.326  0.327 0.981
 8 NP_958784  0.326  0.327 0.981
 9 NP_112598 -0.307 -0.602 2.02 
10 NP_001611  0.458  0.695 1.50 
```
]
.right-column70[
You could replace the highlighted code with


```r
extract(variable, 
        c('ID','Statistic'),
        regex = '(NP_\\d+)_([A-Za-z]+)') %&gt;% 
  pivot_wider(
    names_from=Statistic,
    values_from=value)
```

]

---
class: middle, inverse, center

# Summarizing a data set

---

## Data set summary

There is a function `summary` that will give you summaries of all the variables. It's nice for looking at the data, but the output format isn't very good for further manipulation


```r
summary(brca[,-1]) # Omit first column
```

```
   NP_958782         NP_958785         NP_958786         NP_000436      
 Min.   :-1.9478   Min.   :-1.9527   Min.   :-1.9552   Min.   :-1.9478  
 1st Qu.:-0.4549   1st Qu.:-0.4421   1st Qu.:-0.4440   1st Qu.:-0.4385  
 Median : 0.3237   Median : 0.3270   Median : 0.3270   Median : 0.3303  
 Mean   : 0.3202   Mean   : 0.3269   Mean   : 0.3264   Mean   : 0.3237  
 3rd Qu.: 0.9181   3rd Qu.: 0.9238   3rd Qu.: 0.9238   3rd Qu.: 0.9180  
 Max.   : 2.7651   Max.   : 2.7797   Max.   : 2.7797   Max.   : 2.7980  
   NP_958781         NP_958780         NP_958783         NP_958784      
 Min.   :-1.9576   Min.   :-1.9552   Min.   :-1.9552   Min.   :-1.9552  
 1st Qu.:-0.4440   1st Qu.:-0.4458   1st Qu.:-0.4440   1st Qu.:-0.4440  
 Median : 0.3270   Median : 0.3270   Median : 0.3270   Median : 0.3270  
 Mean   : 0.3271   Mean   : 0.3263   Mean   : 0.3259   Mean   : 0.3260  
 3rd Qu.: 0.9277   3rd Qu.: 0.9238   3rd Qu.: 0.9238   3rd Qu.: 0.9238  
 Max.   : 2.7870   Max.   : 2.7797   Max.   : 2.7834   Max.   : 2.7834  
   NP_112598         NP_001611      
 Min.   :-4.9527   Min.   :-2.5751  
 1st Qu.:-1.6741   1st Qu.:-0.5216  
 Median :-0.6021   Median : 0.6948  
 Mean   :-0.3075   Mean   : 0.4579  
 3rd Qu.: 0.8696   3rd Qu.: 1.4394  
 Max.   : 4.9557   Max.   : 3.4365  
```

---
class: middle, center

# Maybe an easier way?

---

## The `tableone` package

The `tableone` package is meant to create, you guessed it, Table 1. 

It is quite a convenient package for most purposes and saves gobs of time

---

## The `tableone` package

.pull-left[

```r
library(tableone)
tab1 &lt;- CreateTableOne(data=brca[,-1])
tab1
```
]
.pull-right[

```
                       
                        Overall     
  n                        83       
  NP_958782 (mean (SD))  0.32 (0.98)
  NP_958785 (mean (SD))  0.33 (0.98)
  NP_958786 (mean (SD))  0.33 (0.98)
  NP_000436 (mean (SD))  0.32 (0.98)
  NP_958781 (mean (SD))  0.33 (0.98)
  NP_958780 (mean (SD))  0.33 (0.98)
  NP_958783 (mean (SD))  0.33 (0.98)
  NP_958784 (mean (SD))  0.33 (0.98)
  NP_112598 (mean (SD)) -0.31 (2.02)
  NP_001611 (mean (SD))  0.46 (1.50)
```
]

---

## The `tableone` package

.pull-left[

```r
library(tableone)
tab1 &lt;- CreateTableOne(data = brca[-1])
print(tab1, nonnormal = names(brca)[-1])
```

You have to give the variable names of those you think are non-normally distributed and need to be summarized by the median
]
.pull-right[

```
                          
                           Overall            
  n                           83              
  NP_958782 (median [IQR])  0.32 [-0.45, 0.92]
  NP_958785 (median [IQR])  0.33 [-0.44, 0.92]
  NP_958786 (median [IQR])  0.33 [-0.44, 0.92]
  NP_000436 (median [IQR])  0.33 [-0.44, 0.92]
  NP_958781 (median [IQR])  0.33 [-0.44, 0.93]
  NP_958780 (median [IQR])  0.33 [-0.45, 0.92]
  NP_958783 (median [IQR])  0.33 [-0.44, 0.92]
  NP_958784 (median [IQR])  0.33 [-0.44, 0.92]
  NP_112598 (median [IQR]) -0.60 [-1.67, 0.87]
  NP_001611 (median [IQR])  0.69 [-0.52, 1.44]
```
]

---

## The `tableone` package

.pull-left[

```r
library(tableone)
tab1 &lt;- CreateTableOne(data = brca[-1])
kableone(print(tab1, nonnormal = names(brca)[-1]),
         format='html')
```
]
.pull-right[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Overall &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; n &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 83 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958782 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.32 [-0.45, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958785 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.44, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958786 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.44, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_000436 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.44, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958781 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.44, 0.93] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958780 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.45, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958783 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.44, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_958784 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.33 [-0.44, 0.92] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_112598 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.60 [-1.67, 0.87] &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NP_001611 (median [IQR]) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.69 [-0.52, 1.44] &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---
class: middle, center

# Mixed data

---

Let's first put the expression and clinical data together


```r
library(rio)
brca1 &lt;- import('../../data/clinical_data_breast_cancer_hw.csv')
brca2 &lt;- import('../../data/BreastCancer_Expression.csv')
brca &lt;- left_join(brca1, brca2, by=c('Complete.TCGA.ID' = 'TCGA_ID')) %&gt;% 
  mutate(Age.at.Initial.Pathologic.Diagnosis =
           as.numeric(Age.at.Initial.Pathologic.Diagnosis)) %&gt;% 
  mutate(ER.Status = ifelse(ER.Status %in% c('Positive','Negative'),
                            ER.Status, NA))

summary(brca)
```

```
 Complete.TCGA.ID      Gender          Age.at.Initial.Pathologic.Diagnosis
 Length:108         Length:108         Min.   :30.00                      
 Class :character   Class :character   1st Qu.:49.00                      
 Mode  :character   Mode  :character   Median :58.00                      
                                       Mean   :58.72                      
                                       3rd Qu.:66.50                      
                                       Max.   :88.00                      
                                       NA's   :1                          
  ER.Status          PR.Status         HER2.Final.Status     Tumor          
 Length:108         Length:108         Length:108         Length:108        
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
                                                                            
     Node            Metastasis         AJCC.Stage        Vital.Status      
 Length:108         Length:108         Length:108         Length:108        
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
                                                                            
 Days.to.Date.of.Last.Contact Days.to.date.of.Death   NP_958782      
 Min.   :   0.0               Min.   : 160.0        Min.   :-1.9478  
 1st Qu.: 237.5               1st Qu.: 947.5        1st Qu.:-0.4831  
 Median : 654.0               Median :1364.0        Median : 0.3175  
 Mean   : 806.4               Mean   :1254.5        Mean   : 0.3201  
 3rd Qu.:1297.5               3rd Qu.:1627.5        3rd Qu.: 0.9924  
 Max.   :2850.0               Max.   :2483.0        Max.   : 2.7651  
                              NA's   :97            NA's   :28       
   NP_958785         NP_958786         NP_000436         NP_958781      
 Min.   :-1.9527   Min.   :-1.9552   Min.   :-1.9478   Min.   :-1.9576  
 1st Qu.:-0.4804   1st Qu.:-0.4831   1st Qu.:-0.4749   1st Qu.:-0.4832  
 Median : 0.3116   Median : 0.3116   Median : 0.3132   Median : 0.3116  
 Mean   : 0.3267   Mean   : 0.3262   Mean   : 0.3234   Mean   : 0.3267  
 3rd Qu.: 0.9962   3rd Qu.: 0.9962   3rd Qu.: 0.9919   3rd Qu.: 1.0020  
 Max.   : 2.7797   Max.   : 2.7797   Max.   : 2.7980   Max.   : 2.7870  
 NA's   :28        NA's   :28        NA's   :28        NA's   :28       
   NP_958780         NP_958783         NP_958784         NP_112598      
 Min.   :-1.9552   Min.   :-1.9552   Min.   :-1.9552   Min.   :-4.9527  
 1st Qu.:-0.4859   1st Qu.:-0.4831   1st Qu.:-0.4831   1st Qu.:-1.7264  
 Median : 0.3116   Median : 0.3116   Median : 0.3116   Median :-0.6596  
 Mean   : 0.3261   Mean   : 0.3257   Mean   : 0.3258   Mean   :-0.3046  
 3rd Qu.: 0.9962   3rd Qu.: 0.9962   3rd Qu.: 0.9962   3rd Qu.: 1.0183  
 Max.   : 2.7797   Max.   : 2.7834   Max.   : 2.7834   Max.   : 4.9557  
 NA's   :28        NA's   :28        NA's   :28        NA's   :28       
   NP_001611      
 Min.   :-2.5751  
 1st Qu.:-0.5562  
 Median : 0.6084  
 Mean   : 0.3830  
 3rd Qu.: 1.3563  
 Max.   : 3.4365  
 NA's   :28       
```

---

Let's first put the expression and clinical data together


```r
library(rio)
brca1 &lt;- import('../../data/clinical_data_breast_cancer_hw.csv')
brca2 &lt;- import('../../data/BreastCancer_Expression.csv')
brca &lt;- left_join(brca1, brca2, by=c('Complete.TCGA.ID' = 'TCGA_ID')) %&gt;% 
  mutate(Age.at.Initial.Pathologic.Diagnosis =
           as.numeric(Age.at.Initial.Pathologic.Diagnosis)) %&gt;% 
  mutate(ER.Status = ifelse(ER.Status %in% c('Positive','Negative'),
                            ER.Status, NA),
         HER2.Final.Status = ifelse(HER2.Final.Status=='Equivocal', 
                                    NA, HER2.Final.Status)) %&gt;% 
  mutate(across(is.character, as.factor)) %&gt;% 
* mutate(Complete.TCGA.ID = as.character(Complete.TCGA.ID))

str(brca)
```

```
'data.frame':	108 obs. of  23 variables:
 $ Complete.TCGA.ID                   : chr  "TCGA-A2-A0T2" "TCGA-A2-A0CM" "TCGA-BH-A18V" "TCGA-BH-A18Q" ...
 $ Gender                             : Factor w/ 2 levels "FEMALE","MALE": 1 1 1 1 1 1 1 1 1 1 ...
 $ Age.at.Initial.Pathologic.Diagnosis: num  66 40 48 56 38 57 74 60 61 NA ...
 $ ER.Status                          : Factor w/ 2 levels "Negative","Positive": 1 1 1 1 1 1 1 1 1 1 ...
 $ PR.Status                          : Factor w/ 2 levels "Negative","Positive": 1 1 1 1 1 1 1 1 1 1 ...
 $ HER2.Final.Status                  : Factor w/ 2 levels "Negative","Positive": 1 1 1 1 1 1 1 1 1 1 ...
 $ Tumor                              : Factor w/ 4 levels "T1","T2","T3",..: 3 2 2 2 3 2 3 2 2 2 ...
 $ Node                               : Factor w/ 4 levels "N0","N1","N2",..: 4 1 2 2 4 1 1 1 1 1 ...
 $ Metastasis                         : Factor w/ 2 levels "M0","M1": 2 1 1 1 1 1 1 1 1 1 ...
 $ AJCC.Stage                         : Factor w/ 11 levels "Stage I","Stage IA",..: 11 5 6 6 10 5 6 5 5 5 ...
 $ Vital.Status                       : Factor w/ 2 levels "DECEASED","LIVING": 1 1 1 1 2 2 2 2 2 2 ...
 $ Days.to.Date.of.Last.Contact       : int  240 754 1555 1692 133 309 425 643 775 964 ...
 $ Days.to.date.of.Death              : int  240 754 1555 1692 NA NA NA NA NA NA ...
 $ NP_958782                          : num  NA 0.683 NA 0.195 NA ...
 $ NP_958785                          : num  NA 0.694 NA 0.215 NA ...
 $ NP_958786                          : num  NA 0.698 NA 0.215 NA ...
 $ NP_000436                          : num  NA 0.687 NA 0.205 NA ...
 $ NP_958781                          : num  NA 0.687 NA 0.215 NA ...
 $ NP_958780                          : num  NA 0.698 NA 0.215 NA ...
 $ NP_958783                          : num  NA 0.698 NA 0.215 NA ...
 $ NP_958784                          : num  NA 0.698 NA 0.215 NA ...
 $ NP_112598                          : num  NA -2.65 NA -1.04 NA ...
 $ NP_001611                          : num  NA -0.984 NA -0.517 NA ...
```

---

Identify which variables are categorical (factors) and which are continuous (numeric)


```r
catvars &lt;- brca %&gt;% select(where(is.factor)) %&gt;% names()
ctsvars &lt;- brca %&gt;% select(where(is.numeric)) %&gt;% names()
```

---


```r
CreateCatTable(vars = catvars, data = brca)
```

```
                                  
                                   Overall   
  n                                108       
  Gender = MALE (%)                 2 ( 1.9) 
  ER.Status = Positive (%)         69 (64.5) 
  PR.Status = Positive (%)         55 (50.9) 
  HER2.Final.Status = Positive (%) 28 (26.2) 
  Tumor (%)                                  
     T1                            16 (14.8) 
     T2                            67 (62.0) 
     T3                            19 (17.6) 
     T4                             6 ( 5.6) 
  Node (%)                                   
     N0                            54 (50.0) 
     N1                            30 (27.8) 
     N2                            15 (13.9) 
     N3                             9 ( 8.3) 
  Metastasis = M1 (%)               2 ( 1.9) 
  AJCC.Stage (%)                             
     Stage I                        3 ( 2.8) 
     Stage IA                       7 ( 6.5) 
     Stage IB                       2 ( 1.9) 
     Stage II                      11 (10.2) 
     Stage IIA                     32 (29.6) 
     Stage IIB                     23 (21.3) 
     Stage III                      4 ( 3.7) 
     Stage IIIA                    12 (11.1) 
     Stage IIIB                     6 ( 5.6) 
     Stage IIIC                     6 ( 5.6) 
     Stage IV                       2 ( 1.9) 
  Vital.Status = LIVING (%)        97 (89.8) 
```

---


```r
CreateContTable(vars = ctsvars, data = brca)
```

```
                                                 
                                                  Overall         
  n                                               108             
  Age.at.Initial.Pathologic.Diagnosis (mean (SD))   58.72 (13.21) 
  Days.to.Date.of.Last.Contact (mean (SD))         806.37 (667.70)
  Days.to.date.of.Death (mean (SD))               1254.45 (678.05)
  NP_958782 (mean (SD))                              0.32 (0.99)  
  NP_958785 (mean (SD))                              0.33 (1.00)  
  NP_958786 (mean (SD))                              0.33 (1.00)  
  NP_000436 (mean (SD))                              0.32 (0.99)  
  NP_958781 (mean (SD))                              0.33 (1.00)  
  NP_958780 (mean (SD))                              0.33 (1.00)  
  NP_958783 (mean (SD))                              0.33 (1.00)  
  NP_958784 (mean (SD))                              0.33 (1.00)  
  NP_112598 (mean (SD))                             -0.30 (2.06)  
  NP_001611 (mean (SD))                              0.38 (1.46)  
```

---

.pull-left[

```r
brca &lt;- brca %&gt;% 
  rename(
    'Age'='Age.at.Initial.Pathologic.Diagnosis',
    'Last.Contact' = 'Days.to.Date.of.Last.Contact',
    'Death' = 'Days.to.date.of.Death'
  )
ctsvars &lt;- brca %&gt;% 
  select(where(is.numeric))%&gt;% names()
CreateContTable(vars = ctsvars, data = brca)
```
]
.pull-right[

```
                          
                           Overall         
  n                        108             
  Age (mean (SD))            58.72 (13.21) 
  Last.Contact (mean (SD))  806.37 (667.70)
  Death (mean (SD))        1254.45 (678.05)
  NP_958782 (mean (SD))       0.32 (0.99)  
  NP_958785 (mean (SD))       0.33 (1.00)  
  NP_958786 (mean (SD))       0.33 (1.00)  
  NP_000436 (mean (SD))       0.32 (0.99)  
  NP_958781 (mean (SD))       0.33 (1.00)  
  NP_958780 (mean (SD))       0.33 (1.00)  
  NP_958783 (mean (SD))       0.33 (1.00)  
  NP_958784 (mean (SD))       0.33 (1.00)  
  NP_112598 (mean (SD))      -0.30 (2.06)  
  NP_001611 (mean (SD))       0.38 (1.46)  
```
]

---

## Putting it together


```r
CreateTableOne(vars = c(catvars, ctsvars),
               data = brca)
```

```
                                  
                                   Overall         
  n                                    108         
  Gender = MALE (%)                      2 ( 1.9)  
  ER.Status = Positive (%)              69 (64.5)  
  PR.Status = Positive (%)              55 (50.9)  
  HER2.Final.Status = Positive (%)      28 (26.2)  
  Tumor (%)                                        
     T1                                 16 (14.8)  
     T2                                 67 (62.0)  
     T3                                 19 (17.6)  
     T4                                  6 ( 5.6)  
  Node (%)                                         
     N0                                 54 (50.0)  
     N1                                 30 (27.8)  
     N2                                 15 (13.9)  
     N3                                  9 ( 8.3)  
  Metastasis = M1 (%)                    2 ( 1.9)  
  AJCC.Stage (%)                                   
     Stage I                             3 ( 2.8)  
     Stage IA                            7 ( 6.5)  
     Stage IB                            2 ( 1.9)  
     Stage II                           11 (10.2)  
     Stage IIA                          32 (29.6)  
     Stage IIB                          23 (21.3)  
     Stage III                           4 ( 3.7)  
     Stage IIIA                         12 (11.1)  
     Stage IIIB                          6 ( 5.6)  
     Stage IIIC                          6 ( 5.6)  
     Stage IV                            2 ( 1.9)  
  Vital.Status = LIVING (%)             97 (89.8)  
  Age (mean (SD))                    58.72 (13.21) 
  Last.Contact (mean (SD))          806.37 (667.70)
  Death (mean (SD))                1254.45 (678.05)
  NP_958782 (mean (SD))               0.32 (0.99)  
  NP_958785 (mean (SD))               0.33 (1.00)  
  NP_958786 (mean (SD))               0.33 (1.00)  
  NP_000436 (mean (SD))               0.32 (0.99)  
  NP_958781 (mean (SD))               0.33 (1.00)  
  NP_958780 (mean (SD))               0.33 (1.00)  
  NP_958783 (mean (SD))               0.33 (1.00)  
  NP_958784 (mean (SD))               0.33 (1.00)  
  NP_112598 (mean (SD))              -0.30 (2.06)  
  NP_001611 (mean (SD))               0.38 (1.46)  
```

---

## Putting it together


```r
CreateTableOne(data = brca[,-1])
```

```
                                  
                                   Overall         
  n                                    108         
  Gender = MALE (%)                      2 ( 1.9)  
  Age (mean (SD))                    58.72 (13.21) 
  ER.Status = Positive (%)              69 (64.5)  
  PR.Status = Positive (%)              55 (50.9)  
  HER2.Final.Status = Positive (%)      28 (26.2)  
  Tumor (%)                                        
     T1                                 16 (14.8)  
     T2                                 67 (62.0)  
     T3                                 19 (17.6)  
     T4                                  6 ( 5.6)  
  Node (%)                                         
     N0                                 54 (50.0)  
     N1                                 30 (27.8)  
     N2                                 15 (13.9)  
     N3                                  9 ( 8.3)  
  Metastasis = M1 (%)                    2 ( 1.9)  
  AJCC.Stage (%)                                   
     Stage I                             3 ( 2.8)  
     Stage IA                            7 ( 6.5)  
     Stage IB                            2 ( 1.9)  
     Stage II                           11 (10.2)  
     Stage IIA                          32 (29.6)  
     Stage IIB                          23 (21.3)  
     Stage III                           4 ( 3.7)  
     Stage IIIA                         12 (11.1)  
     Stage IIIB                          6 ( 5.6)  
     Stage IIIC                          6 ( 5.6)  
     Stage IV                            2 ( 1.9)  
  Vital.Status = LIVING (%)             97 (89.8)  
  Last.Contact (mean (SD))          806.37 (667.70)
  Death (mean (SD))                1254.45 (678.05)
  NP_958782 (mean (SD))               0.32 (0.99)  
  NP_958785 (mean (SD))               0.33 (1.00)  
  NP_958786 (mean (SD))               0.33 (1.00)  
  NP_000436 (mean (SD))               0.32 (0.99)  
  NP_958781 (mean (SD))               0.33 (1.00)  
  NP_958780 (mean (SD))               0.33 (1.00)  
  NP_958783 (mean (SD))               0.33 (1.00)  
  NP_958784 (mean (SD))               0.33 (1.00)  
  NP_112598 (mean (SD))              -0.30 (2.06)  
  NP_001611 (mean (SD))               0.38 (1.46)  
```


---

class: middle, center

# Grouped summaries

---

.left-column70[

```r
brca %&gt;% 
  group_by(ER.Status) %&gt;% 
  summarize(across(starts_with('NP'),
               mean))
```

```
# A tibble: 3 x 11
  ER.Status NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780
* &lt;fct&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Negative         NA        NA        NA        NA        NA        NA
2 Positive         NA        NA        NA        NA        NA        NA
3 &lt;NA&gt;             NA        NA        NA        NA        NA        NA
  NP_958783 NP_958784 NP_112598 NP_001611
*     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1        NA        NA        NA        NA
2        NA        NA        NA        NA
3        NA        NA        NA        NA
```
]
.right-column70[
There are missing values now, so we have to use `na.rm=T`.
]

---

.left-column70[

```r
brca %&gt;% 
  group_by(ER.Status) %&gt;% 
  summarize(across(starts_with('NP'),
*              mean, na.rm=T))
```

```
# A tibble: 3 x 11
  ER.Status NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780
* &lt;fct&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Negative      0.429     0.438     0.439     0.432     0.436     0.436
2 Positive      0.267     0.273     0.272     0.271     0.274     0.273
3 &lt;NA&gt;        NaN       NaN       NaN       NaN       NaN       NaN    
  NP_958783 NP_958784 NP_112598 NP_001611
*     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1     0.436     0.436    -0.197    -0.566
2     0.272     0.273    -0.357     0.840
3   NaN       NaN       NaN       NaN    
```
]
.right-column70[
We still have a row for the missing values of ER.Status
]

---

.left-column70[

```r
brca %&gt;% 
* filter(!is.na(ER.Status)) %&gt;%
  group_by(ER.Status) %&gt;% 
  summarize(across(starts_with('NP'),
               mean, na.rm=T))
```

```
# A tibble: 2 x 11
  ER.Status NP_958782 NP_958785 NP_958786 NP_000436 NP_958781 NP_958780
* &lt;fct&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 Negative      0.429     0.438     0.439     0.432     0.436     0.436
2 Positive      0.267     0.273     0.272     0.271     0.274     0.273
  NP_958783 NP_958784 NP_112598 NP_001611
*     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1     0.436     0.436    -0.197    -0.566
2     0.272     0.273    -0.357     0.840
```
]
.right-column70[
How about reversing the rows and columns for readability

]

---

.pull-left[

```r
brca %&gt;% 
  filter(!is.na(ER.Status)) %&gt;%
  group_by(ER.Status) %&gt;% 
  summarize(across(starts_with('NP'),
               mean, na.rm=T)) %&gt;% 
* pivot_longer(names_to='ID', values_to='value',
*              cols = c(-ER.Status)) %&gt;%
* pivot_wider(names_from = ER.Status,
*             values_from=value)
```
]
.pull-right[

```
# A tibble: 10 x 3
   ID        Negative Positive
   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;
 1 NP_958782    0.429    0.267
 2 NP_958785    0.438    0.273
 3 NP_958786    0.439    0.272
 4 NP_000436    0.432    0.271
 5 NP_958781    0.436    0.274
 6 NP_958780    0.436    0.273
 7 NP_958783    0.436    0.272
 8 NP_958784    0.436    0.273
 9 NP_112598   -0.197   -0.357
10 NP_001611   -0.566    0.840
```
]

---

### Using `tableone`


```r
CreateTableOne(
  data = brca %&gt;% filter(!is.na(ER.Status)),
  vars = brca %&gt;% 
    select(starts_with('NP')) %&gt;% 
    names(),
  strata = 'ER.Status', # single quotes, not backticks
  test = F)
```

```
                       Stratified by ER.Status
                        Negative     Positive    
  n                        38           69       
  NP_958782 (mean (SD))  0.43 (1.13)  0.27 (0.93)
  NP_958785 (mean (SD))  0.44 (1.14)  0.27 (0.93)
  NP_958786 (mean (SD))  0.44 (1.14)  0.27 (0.93)
  NP_000436 (mean (SD))  0.43 (1.14)  0.27 (0.93)
  NP_958781 (mean (SD))  0.44 (1.14)  0.27 (0.93)
  NP_958780 (mean (SD))  0.44 (1.14)  0.27 (0.93)
  NP_958783 (mean (SD))  0.44 (1.14)  0.27 (0.93)
  NP_958784 (mean (SD))  0.44 (1.14)  0.27 (0.93)
  NP_112598 (mean (SD)) -0.20 (2.28) -0.36 (1.97)
  NP_001611 (mean (SD)) -0.57 (1.54)  0.84 (1.19)
```

---

## Alternatives to **tableone**

+ [table1](https://github.com/benjaminrich/table1)
+ [gtsummary](https://cran.r-project.org/package=gtsummary)
+ [flextable](https://davidgohel.github.io/flextable/)
+ [arsenal](https://github.com/eheinzen/arsenal)

---

## arsenal


```r
library(arsenal)
summary(tableby(ER.Status ~ ., data = brca[,-1])) # Here . implies all other variables.
```



|                             |  Negative (N=38)   |  Positive (N=69)   |   Total (N=107)    | p value|
|:----------------------------|:------------------:|:------------------:|:------------------:|-------:|
|**Gender**                   |                    |                    |                    |   0.289|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;FEMALE     |    38 (100.0%)     |     67 (97.1%)     |    105 (98.1%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;MALE       |      0 (0.0%)      |      2 (2.9%)      |      2 (1.9%)      |        |
|**Age**                      |                    |                    |                    |   0.101|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         1          |         0          |         1          |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |  55.919 (12.269)   |  60.348 (13.573)   |  58.802 (13.245)   |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |  36.000 - 82.000   |  30.000 - 88.000   |  30.000 - 88.000   |        |
|**PR.Status**                |                    |                    |                    | &lt; 0.001|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Negative   |    38 (100.0%)     |     14 (20.3%)     |     52 (48.6%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Positive   |      0 (0.0%)      |     55 (79.7%)     |     55 (51.4%)     |        |
|**HER2.Final.Status**        |                    |                    |                    |   0.281|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         0          |         1          |         1          |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Negative   |     26 (68.4%)     |     53 (77.9%)     |     79 (74.5%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Positive   |     12 (31.6%)     |     15 (22.1%)     |     27 (25.5%)     |        |
|**Tumor**                    |                    |                    |                    |   0.553|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;T1         |     6 (15.8%)      |     10 (14.5%)     |     16 (15.0%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;T2         |     26 (68.4%)     |     40 (58.0%)     |     66 (61.7%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;T3         |     5 (13.2%)      |     14 (20.3%)     |     19 (17.8%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;T4         |      1 (2.6%)      |      5 (7.2%)      |      6 (5.6%)      |        |
|**Node**                     |                    |                    |                    |   0.685|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N0         |     22 (57.9%)     |     32 (46.4%)     |     54 (50.5%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N1         |     8 (21.1%)      |     21 (30.4%)     |     29 (27.1%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N2         |     5 (13.2%)      |     10 (14.5%)     |     15 (14.0%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N3         |      3 (7.9%)      |      6 (8.7%)      |      9 (8.4%)      |        |
|**Metastasis**               |                    |                    |                    |   0.666|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;M0         |     37 (97.4%)     |     68 (98.6%)     |    105 (98.1%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;M1         |      1 (2.6%)      |      1 (1.4%)      |      2 (1.9%)      |        |
|**AJCC.Stage**               |                    |                    |                    |   0.510|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage I    |      1 (2.6%)      |      2 (2.9%)      |      3 (2.8%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IA   |      1 (2.6%)      |      6 (8.7%)      |      7 (6.5%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IB   |      0 (0.0%)      |      1 (1.4%)      |      1 (0.9%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage II   |     4 (10.5%)      |     7 (10.1%)      |     11 (10.3%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IIA  |     17 (44.7%)     |     15 (21.7%)     |     32 (29.9%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IIB  |     6 (15.8%)      |     17 (24.6%)     |     23 (21.5%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage III  |      2 (5.3%)      |      2 (2.9%)      |      4 (3.7%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IIIA |      3 (7.9%)      |     9 (13.0%)      |     12 (11.2%)     |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IIIB |      1 (2.6%)      |      5 (7.2%)      |      6 (5.6%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IIIC |      2 (5.3%)      |      4 (5.8%)      |      6 (5.6%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Stage IV   |      1 (2.6%)      |      1 (1.4%)      |      2 (1.9%)      |        |
|**Vital.Status**             |                    |                    |                    |   0.756|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;DECEASED   |     4 (10.5%)      |      6 (8.7%)      |     10 (9.3%)      |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;LIVING     |     34 (89.5%)     |     63 (91.3%)     |     97 (90.7%)     |        |
|**Last.Contact**             |                    |                    |                    |   0.755|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  | 776.184 (700.000)  | 818.754 (658.004)  | 803.636 (670.232)  |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |  0.000 - 2426.000  |  0.000 - 2850.000  |  0.000 - 2850.000  |        |
|**Death**                    |                    |                    |                    |   0.490|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         34         |         63         |         97         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  | 1060.250 (685.683) | 1402.833 (760.422) | 1265.800 (713.627) |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      | 240.000 - 1692.000 | 160.000 - 2483.000 | 160.000 - 2483.000 |        |
|**NP_958782**                |                    |                    |                    |   0.498|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.429 (1.128)    |   0.267 (0.927)    |   0.320 (0.993)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.948 - 2.707   |   -1.523 - 2.765   |   -1.948 - 2.765   |        |
|**NP_958785**                |                    |                    |                    |   0.492|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.438 (1.137)    |   0.273 (0.927)    |   0.327 (0.996)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.953 - 2.734   |   -1.528 - 2.780   |   -1.953 - 2.780   |        |
|**NP_958786**                |                    |                    |                    |   0.487|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.439 (1.137)    |   0.272 (0.927)    |   0.326 (0.996)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.955 - 2.738   |   -1.528 - 2.780   |   -1.955 - 2.780   |        |
|**NP_000436**                |                    |                    |                    |   0.502|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.432 (1.136)    |   0.271 (0.926)    |   0.323 (0.994)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.948 - 2.734   |   -1.531 - 2.798   |   -1.948 - 2.798   |        |
|**NP_958781**                |                    |                    |                    |   0.499|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.436 (1.139)    |   0.274 (0.927)    |   0.327 (0.997)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.958 - 2.753   |   -1.514 - 2.787   |   -1.958 - 2.787   |        |
|**NP_958780**                |                    |                    |                    |   0.496|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.436 (1.136)    |   0.273 (0.927)    |   0.326 (0.995)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.955 - 2.738   |   -1.525 - 2.780   |   -1.955 - 2.780   |        |
|**NP_958783**                |                    |                    |                    |   0.495|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.436 (1.138)    |   0.272 (0.928)    |   0.326 (0.997)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.955 - 2.738   |   -1.525 - 2.783   |   -1.955 - 2.783   |        |
|**NP_958784**                |                    |                    |                    |   0.495|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   0.436 (1.138)    |   0.273 (0.928)    |   0.326 (0.997)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -1.955 - 2.738   |   -1.525 - 2.783   |   -1.955 - 2.783   |        |
|**NP_112598**                |                    |                    |                    |   0.748|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   -0.197 (2.283)   |   -0.357 (1.968)   |   -0.305 (2.062)   |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -3.071 - 4.090   |   -4.953 - 4.956   |   -4.953 - 4.956   |        |
|**NP_001611**                |                    |                    |                    | &lt; 0.001|
|&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss     |         12         |         15         |         27         |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD)  |   -0.566 (1.542)   |   0.840 (1.193)    |   0.383 (1.465)    |        |
|&amp;nbsp;&amp;nbsp;&amp;nbsp;Range      |   -2.575 - 2.838   |   -2.175 - 3.436   |   -2.575 - 3.436   |        |




---
class: middle,center,inverse

# Hypothesis tests

---
class: middle, center

# Comparing two groups



---

## The t-test

The t-test compares whether the mean of a variable differs between two groups. 

It does assume the normal distribution for the data, but is robust to deviations from normality

Do **not** test for normality before doing the t-test. It isn't necessary and screws up your error rates

---

## The t-test

In R, there is a convenient function `t.test`


```r
t.test(NP_958782 ~ ER.Status, data = brca)
```

```

	Welch Two Sample t-test

data:  NP_958782 by ER.Status
t = 0.63522, df = 41.807, p-value = 0.5287
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.3523151  0.6759226
sample estimates:
mean in group Negative mean in group Positive 
             0.4292798              0.2674761 
```

Read the code as 

"Do a t-test to see if (the mean of) `NP_958782` differs by `ER.Status`, where
both are taken from the data set `brca`"

You can read the `~` as "by", as in "t-test of NP_958782 by ER.Status"

---

## The t-test

The packge `broom` provides a function `tidy` that makes the results of these
statistical tests tidy.


```r
t.test(NP_958782 ~ ER.Status, data=brca) %&gt;% 
  broom::tidy()
```

```
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.162     0.429     0.267     0.635   0.529      41.8   -0.352     0.676
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  
```

--


```

	Welch Two Sample t-test

data:  NP_958782 by ER.Status
t = 0.63522, df = 41.807, p-value = 0.5287
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.3523151  0.6759226
sample estimates:
mean in group Negative mean in group Positive 
             0.4292798              0.2674761 
```

---



count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
*brca
```
]
 
.panel2-through-auto[

```
    Complete.TCGA.ID Gender Age.at.Initial.Pathologic.Diagnosis ER.Status
1       TCGA-A2-A0T2 FEMALE                                  66  Negative
2       TCGA-A2-A0CM FEMALE                                  40  Negative
3       TCGA-BH-A18V FEMALE                                  48  Negative
4       TCGA-BH-A18Q FEMALE                                  56  Negative
5       TCGA-BH-A0E0 FEMALE                                  38  Negative
6       TCGA-A7-A0CE FEMALE                                  57  Negative
7       TCGA-D8-A142 FEMALE                                  74  Negative
8       TCGA-A2-A0D0 FEMALE                                  60  Negative
9       TCGA-AO-A0J6 FEMALE                                  61  Negative
10      TCGA-A2-A0YM FEMALE                                  NA  Negative
11      TCGA-A2-A0D2 FEMALE                                  45  Negative
12      TCGA-A2-A0SX FEMALE                                  48  Negative
13      TCGA-AO-A0JL FEMALE                                  59  Negative
14      TCGA-AO-A12F FEMALE                                  36  Negative
15      TCGA-AN-A0AL FEMALE                                  41  Negative
16      TCGA-AN-A0FL FEMALE                                  62  Negative
17      TCGA-AR-A0U4 FEMALE                                  54  Negative
18      TCGA-AR-A1AQ FEMALE                                  49  Negative
19      TCGA-BH-A0AV FEMALE                                  52  Negative
20      TCGA-C8-A12V FEMALE                                  55  Negative
21      TCGA-C8-A131 FEMALE                                  82  Negative
22      TCGA-C8-A131 FEMALE                                  82  Negative
23      TCGA-C8-A134 FEMALE                                  52  Negative
24      TCGA-E2-A150 FEMALE                                  48  Negative
25      TCGA-E2-A158 FEMALE                                  43  Negative
26      TCGA-E2-A159 FEMALE                                  50  Negative
27      TCGA-BH-A18R FEMALE                                  50      &lt;NA&gt;
28      TCGA-A2-A0T1 FEMALE                                  55  Negative
29      TCGA-BH-A0EE FEMALE                                  68  Negative
30      TCGA-A2-A0D1 FEMALE                                  76  Negative
31      TCGA-AO-A12D FEMALE                                  43  Negative
32      TCGA-AO-A12D FEMALE                                  43  Negative
33      TCGA-AO-A0JE FEMALE                                  53  Negative
34      TCGA-A2-A0EQ FEMALE                                  64  Negative
35      TCGA-A8-A076 FEMALE                                  66  Positive
36      TCGA-A8-A09G FEMALE                                  79  Positive
37      TCGA-AR-A0TX FEMALE                                  64  Positive
38      TCGA-C8-A12L FEMALE                                  67  Negative
39      TCGA-C8-A12P FEMALE                                  55  Negative
40      TCGA-C8-A12Q FEMALE                                  78  Negative
41      TCGA-C8-A12T FEMALE                                  43  Positive
42      TCGA-C8-A12Z FEMALE                                  45  Negative
43      TCGA-C8-A130 FEMALE                                  52  Positive
44      TCGA-C8-A135 FEMALE                                  64  Negative
45      TCGA-C8-A138 FEMALE                                  54  Positive
46      TCGA-AR-A0TR FEMALE                                  68  Positive
47      TCGA-BH-A18N FEMALE                                  88  Positive
48      TCGA-BH-A0HK FEMALE                                  81  Positive
49      TCGA-A7-A0CD FEMALE                                  66  Positive
50      TCGA-BH-A0HP FEMALE                                  65  Positive
51      TCGA-A2-A0YI FEMALE                                  62  Positive
52      TCGA-A2-A0YL FEMALE                                  48  Positive
53      TCGA-A2-A0YF FEMALE                                  67  Positive
54      TCGA-BH-A0E1 FEMALE                                  52  Positive
55      TCGA-A2-A0T6 FEMALE                                  50  Positive
56      TCGA-A2-A0T7 FEMALE                                  51  Positive
57      TCGA-A2-A0YD FEMALE                                  63  Positive
58      TCGA-A2-A0EV FEMALE                                  80  Positive
59      TCGA-A2-A0YC FEMALE                                  59  Positive
60      TCGA-AO-A0J9 FEMALE                                  61  Positive
61      TCGA-BH-A0BV FEMALE                                  78  Positive
62      TCGA-AO-A12E FEMALE                                  51  Positive
63      TCGA-AO-A126 FEMALE                                  39  Positive
64      TCGA-A2-A0EX FEMALE                                  46  Positive
65      TCGA-AO-A0JJ FEMALE                                  54  Positive
66      TCGA-A8-A08Z FEMALE                                  76  Positive
67      TCGA-AN-A04A FEMALE                                  36  Positive
68      TCGA-AR-A1AP FEMALE                                  80  Positive
69      TCGA-AR-A1AS FEMALE                                  54  Positive
70      TCGA-AR-A1AW FEMALE                                  65  Positive
71      TCGA-BH-A0C1 FEMALE                                  61  Positive
72      TCGA-BH-A0DG FEMALE                                  30  Positive
73      TCGA-BH-A0E9 FEMALE                                  53  Positive
74      TCGA-E2-A154 FEMALE                                  68  Positive
75      TCGA-AO-A03O FEMALE                                  69  Positive
76      TCGA-A2-A0SW FEMALE                                  82  Positive
77      TCGA-BH-A18U FEMALE                                  72  Positive
78      TCGA-AR-A0TY FEMALE                                  54  Positive
79      TCGA-D8-A13Y FEMALE                                  52  Positive
80      TCGA-A7-A13F FEMALE                                  44  Positive
81      TCGA-A7-A0CJ FEMALE                                  57  Positive
82      TCGA-A2-A0T3 FEMALE                                  37  Positive
83      TCGA-A2-A0YG FEMALE                                  63  Positive
84      TCGA-A2-A0EY FEMALE                                  62  Positive
85      TCGA-E2-A10A FEMALE                                  41  Positive
86      TCGA-BH-A0C0 FEMALE                                  62  Positive
87      TCGA-AO-A0JC FEMALE                                  64  Positive
88      TCGA-AO-A0JM FEMALE                                  40  Positive
89      TCGA-AO-A12B FEMALE                                  63  Positive
90      TCGA-AO-A12B FEMALE                                  63  Positive
91      TCGA-A8-A06N FEMALE                                  66  Positive
92      TCGA-A8-A06Z FEMALE                                  84  Positive
93      TCGA-A8-A079 FEMALE                                  69  Positive
94      TCGA-A8-A08G FEMALE                                  41  Positive
95      TCGA-A8-A09I FEMALE                                  84  Positive
96      TCGA-AN-A0AJ FEMALE                                  79  Positive
97      TCGA-AN-A0AM FEMALE                                  56  Positive
98      TCGA-AN-A0AS FEMALE                                  70  Positive
99      TCGA-AN-A0FK FEMALE                                  88  Positive
100     TCGA-AR-A0TT FEMALE                                  53  Positive
101     TCGA-AR-A0TV FEMALE                                  66  Positive
102     TCGA-AR-A1AV   MALE                                  68  Positive
103     TCGA-BH-A0BZ FEMALE                                  59  Positive
104     TCGA-BH-A0C7 FEMALE                                  48  Positive
105     TCGA-BH-A0DD   MALE                                  58  Positive
106     TCGA-C8-A12U FEMALE                                  46  Positive
107     TCGA-C8-A12W FEMALE                                  49  Positive
108     TCGA-E2-A15A FEMALE                                  45  Positive
    PR.Status HER2.Final.Status Tumor Node Metastasis AJCC.Stage Vital.Status
1    Negative          Negative    T3   N3         M1   Stage IV     DECEASED
2    Negative          Negative    T2   N0         M0  Stage IIA     DECEASED
3    Negative          Negative    T2   N1         M0  Stage IIB     DECEASED
4    Negative          Negative    T2   N1         M0  Stage IIB     DECEASED
5    Negative          Negative    T3   N3         M0 Stage IIIC       LIVING
6    Negative          Negative    T2   N0         M0  Stage IIA       LIVING
7    Negative          Negative    T3   N0         M0  Stage IIB       LIVING
8    Negative          Negative    T2   N0         M0  Stage IIA       LIVING
9    Negative          Negative    T2   N0         M0  Stage IIA       LIVING
10   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
11   Negative          Negative    T2   N0         M0  Stage IIB       LIVING
12   Negative          Negative    T1   N0         M0   Stage IA       LIVING
13   Negative          Negative    T2   N2         M0 Stage IIIA       LIVING
14   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
15   Negative          Negative    T4   N0         M0 Stage IIIB       LIVING
16   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
17   Negative          Negative    T2   N0         M0   Stage II       LIVING
18   Negative          Negative    T2   N0         M0   Stage II       LIVING
19   Negative          Negative    T1   N0         M0    Stage I       LIVING
20   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
21   Negative          Negative    T2   N2         M0  Stage III       LIVING
22   Negative          Negative    T2   N2         M0  Stage III       LIVING
23   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
24   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
25   Negative          Negative    T1   N1         M0  Stage IIA       LIVING
26   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
27   Negative          Positive    T2   N1         M0   Stage IB     DECEASED
28   Negative          Positive    T3   N3         M0 Stage IIIC       LIVING
29   Negative          Positive    T3   N0         M0  Stage IIB       LIVING
30   Negative          Positive    T2   N0         M0  Stage IIA       LIVING
31   Negative          Positive    T1   N1         M0  Stage IIA       LIVING
32   Negative          Positive    T1   N1         M0  Stage IIA       LIVING
33   Negative          Positive    T2   N2         M0 Stage IIIA       LIVING
34   Negative          Positive    T2   N0         M0  Stage IIA       LIVING
35   Positive          Positive    T2   N0         M0  Stage IIA       LIVING
36   Negative          Positive    T3   N3         M0 Stage IIIC       LIVING
37   Positive          Positive    T1   N1         M0   Stage II       LIVING
38   Negative          Positive    T2   N0         M0  Stage IIA       LIVING
39   Negative          Positive    T2   N1         M0  Stage IIB       LIVING
40   Negative          Positive    T1   N2         M0 Stage IIIA       LIVING
41   Positive          Positive    T2   N0         M0  Stage IIA       LIVING
42   Negative          Positive    T2   N1         M0   Stage II       LIVING
43   Positive         Equivocal    T3   N2         M0 Stage IIIA       LIVING
44   Negative          Positive    T2   N1         M0   Stage II       LIVING
45   Negative          Positive    T2   N2         M0  Stage III       LIVING
46   Positive          Negative    T2   N1         M0   Stage II     DECEASED
47   Positive          Negative    T2   N1         M0  Stage IIA     DECEASED
48   Negative          Negative    T2   N1         M0  Stage IIB       LIVING
49   Positive          Negative    T1   N0         M0   Stage IA       LIVING
50   Negative          Negative    T3   N2         M0 Stage IIIA       LIVING
51   Positive          Negative    T1   N0         M0   Stage IA       LIVING
52   Positive          Negative    T3   N2         M0 Stage IIIA       LIVING
53   Negative          Negative    T1   N0         M0   Stage IA       LIVING
54   Positive          Negative    T2   N1         M0  Stage IIB       LIVING
55   Positive          Negative    T3   N0         M0  Stage IIB       LIVING
56   Positive          Negative    T2   N0         M0  Stage IIA       LIVING
57   Positive          Negative    T3   N0         M0  Stage IIB       LIVING
58   Positive          Negative    T1   N0         M0   Stage IA       LIVING
59   Positive          Negative    T2   N1         M0  Stage IIB       LIVING
60   Positive          Negative    T2   N3         M0 Stage IIIC       LIVING
61   Positive          Negative    T2   N1         M0  Stage IIB       LIVING
62   Positive          Negative    T3   N0         M0  Stage IIB       LIVING
63   Positive          Negative    T2   N0         M0  Stage IIA       LIVING
64   Positive          Negative    T3   N0         M0  Stage IIB       LIVING
65   Positive          Negative    T2   N1         M0  Stage IIB       LIVING
66   Positive          Negative    T4   N3         M0 Stage IIIB       LIVING
67   Positive          Negative    T2   N2         M0 Stage IIIA       LIVING
68   Positive          Negative    T1   N0         M0    Stage I       LIVING
69   Positive          Negative    T2   N1         M0   Stage II       LIVING
70   Positive          Negative    T2   N0         M0   Stage II       LIVING
71   Positive          Negative    T3   N2         M0 Stage IIIA       LIVING
72   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
73   Positive          Negative    T2   N1         M0  Stage IIB       LIVING
74   Positive          Negative    T1   N0         M0    Stage I       LIVING
75   Positive          Negative    T2   N0         M0  Stage IIA     DECEASED
76   Negative          Negative    T2   N2         M1   Stage IV     DECEASED
77   Positive          Positive    T2   N2         M0 Stage IIIA     DECEASED
78   Negative          Negative    T2   N0         M0   Stage II     DECEASED
79   Positive          Negative    T1   N0         M0   Stage IA       LIVING
80   Positive          Negative    T3   N1         M0 Stage IIIA       LIVING
81   Positive          Negative    T2   N0         M0  Stage IIA       LIVING
82   Positive          Negative    T1   N1         M0   Stage IA       LIVING
83   Positive          Positive    T2   N3         M0 Stage IIIC       LIVING
84   Negative          Positive    T2   N1         M0  Stage IIB       LIVING
85   Positive          Negative    T3   N0         M0  Stage IIB       LIVING
86   Positive          Positive    T1   N1         M0  Stage IIA       LIVING
87   Positive          Negative    T2   N0         M0  Stage IIA       LIVING
88   Positive          Positive    T2   N1         M0  Stage IIB       LIVING
89   Positive          Negative    T2   N0         M0  Stage IIA       LIVING
90   Positive          Negative    T2   N0         M0  Stage IIA       LIVING
91   Negative          Negative    T4   N0         M0 Stage IIIB       LIVING
92   Positive          Negative    T3   N0         M0  Stage IIB       LIVING
93   Positive          Negative    T4   N3         M0 Stage IIIB       LIVING
94   Positive          Positive    T2   N0         M0  Stage IIA       LIVING
95   Positive          Positive    T2   N0         M0  Stage IIA       LIVING
96   Positive          Positive    T3   N0         M0  Stage IIB       LIVING
97   Negative          Negative    T2   N0         M0  Stage IIA       LIVING
98   Negative          Negative    T2   N2         M0 Stage IIIA       LIVING
99   Positive          Negative    T4   N0         M0 Stage IIIB       LIVING
100  Negative          Negative    T2   N2         M0  Stage III       LIVING
101  Positive          Negative    T2   N0         M0   Stage II       LIVING
102  Positive          Negative    T2   N1         M0   Stage II       LIVING
103  Positive          Negative    T3   N1         M0 Stage IIIA       LIVING
104  Negative          Positive    T2   N1         M0  Stage IIB       LIVING
105  Positive          Positive    T2   N1         M0  Stage IIB       LIVING
106  Positive          Negative    T2   N1         M0   Stage IB       LIVING
107  Positive          Negative    T4   N1         M0 Stage IIIB       LIVING
108  Positive          Negative    T2   N3         M0 Stage IIIC       LIVING
    Days.to.Date.of.Last.Contact Days.to.date.of.Death   NP_958782   NP_958785
1                            240                   240          NA          NA
2                            754                   754  0.68340354  0.69442409
3                           1555                  1555          NA          NA
4                           1692                  1692  0.19534065  0.21541294
5                            133                    NA          NA          NA
6                            309                    NA -1.12317308 -1.12317308
7                            425                    NA  0.53859578  0.54221052
8                            643                    NA          NA          NA
9                            775                    NA  0.83113175  0.85653983
10                           964                    NA  0.65584968  0.65814259
11                          1027                    NA  0.10749090  0.10416449
12                          1288                    NA -0.39855983 -0.39260141
13                          1319                    NA -0.10667998 -0.10667998
14                          1471                    NA -1.94779243 -1.95271766
15                           197                    NA  0.32366271  0.32697264
16                           230                    NA  2.45513793  2.48013666
17                          1627                    NA -0.03322133 -0.03021642
18                          1309                    NA          NA          NA
19                          1180                    NA  0.35053566  0.36740533
20                             0                    NA  0.67390470  0.68871756
21                             0                    NA  2.60994298  2.65042179
22                             0                    NA  2.70725015  2.73383192
23                             0                    NA  0.14018179  0.12605376
24                           591                    NA          NA          NA
25                           450                    NA -1.08652907 -1.09549238
26                           515                    NA          NA          NA
27                          1099                  1141          NA          NA
28                           520                    NA          NA          NA
29                           943                    NA          NA          NA
30                          1051                    NA          NA          NA
31                          1948                    NA  1.09613118  1.11137037
32                          1948                    NA  1.10068807  1.10068807
33                          1965                    NA  0.55977697  0.56340687
34                          2426                    NA -0.91267028 -0.92797866
35                          1641                    NA  1.87398183  1.87038310
36                             0                    NA -1.52334349 -1.51264622
37                            15                    NA -0.58342864 -0.57254887
38                             0                    NA  1.40757026  1.40757026
39                             0                    NA          NA          NA
40                             0                    NA          NA          NA
41                             0                    NA -0.20491791 -0.16241849
42                             0                    NA -0.78719498 -0.75594056
43                             0                    NA -0.49405958 -0.50389919
44                             0                    NA  1.12050183  1.13761779
45                             7                    NA  2.76508066  2.77970922
46                            21                   160 -1.10167521 -1.10878258
47                          1148                  1148  1.10126053  1.10126053
48                           178                    NA          NA          NA
49                           372                    NA          NA          NA
50                           414                    NA          NA          NA
51                           441                    NA          NA          NA
52                           445                    NA          NA          NA
53                           469                    NA  0.31131919  0.29617712
54                           477                    NA  0.76204441  0.76204441
55                           575                    NA  0.79397556  0.81818151
56                           631                    NA          NA          NA
57                           769                    NA  0.06377853  0.09333637
58                           968                    NA  0.45298593  0.47259013
59                           989                    NA  0.81882406  0.81487725
60                          1255                    NA  0.02000705  0.01195532
61                          1519                    NA -0.06583842 -0.05589267
62                          1742                    NA  0.26485911  0.27571131
63                          2850                    NA  0.19587669  0.19587669
64                           549                    NA  1.18510823  1.19261195
65                          1512                    NA  0.75718813  0.78087066
66                          1217                    NA  0.56889458  0.56889458
67                            89                    NA  0.38458773  0.37139283
68                          1215                    NA  0.46108752  0.46108752
69                          1242                    NA  1.22250730  1.21897360
70                          1072                    NA  0.57830868  0.58221285
71                          1338                    NA -0.51836650 -0.51000200
72                           713                    NA -0.20563748 -0.20563748
73                          1405                    NA  1.46666516  1.48228346
74                           325                    NA  0.86265926  0.87018604
75                           993                  2483  1.05322494  1.05594815
76                          1364                  1364 -0.48777247 -0.48777247
77                          1317                  1563 -0.26503035 -0.25164229
78                           544                  1699          NA          NA
79                           362                    NA          NA          NA
80                           387                    NA  1.27918472  1.27516713
81                           409                    NA -1.00123984 -1.00461982
82                           569                    NA  0.58371325  0.58062313
83                           665                    NA -0.07820182 -0.06805814
84                           735                    NA  1.17488099  1.18320879
85                          1229                    NA          NA          NA
86                          1270                    NA          NA          NA
87                          1547                    NA -0.30726684 -0.30726684
88                          1826                    NA  1.39524718  1.40892213
89                          2359                    NA -0.65982805 -0.64874215
90                          2359                    NA -0.96390393 -0.93820953
91                             0                    NA  0.23854713  0.24981815
92                            31                    NA -0.49640909 -0.49850890
93                           274                    NA  1.04895925  1.05225694
94                           606                    NA          NA          NA
95                          1006                    NA          NA          NA
96                           243                    NA -0.42818149 -0.40637805
97                             5                    NA  1.50196696  1.51034838
98                             9                    NA          NA          NA
99                           212                    NA  0.97356417  0.97747606
100                         1679                    NA -0.51142119 -0.52606668
101                          904                    NA -1.51427833 -1.52828543
102                         1295                    NA -0.75982306 -0.75982306
103                         1492                    NA          NA          NA
104                         1305                    NA -0.55221197 -0.54774937
105                         1393                    NA -0.69231577 -0.65946866
106                            0                    NA -0.48155015 -0.47788983
107                            0                    NA          NA          NA
108                          502                    NA  2.18012330  2.18012330
      NP_958786    NP_000436   NP_958781   NP_958780   NP_958783   NP_958784
1            NA           NA          NA          NA          NA          NA
2    0.69809760  0.687077054  0.68707705  0.69809760  0.69809760  0.69809760
3            NA           NA          NA          NA          NA          NA
4    0.21541294  0.205376799  0.21541294  0.21541294  0.21541294  0.21541294
5            NA           NA          NA          NA          NA          NA
6   -1.11686051 -1.129485657 -1.12948566 -1.12001680 -1.12317308 -1.12317308
7    0.54221052  0.534981038  0.54221052  0.54221052  0.54221052  0.54221052
8            NA           NA          NA          NA          NA          NA
9    0.85653983  0.836777987  0.86500919  0.85653983  0.85089359  0.85089359
10   0.65584968  0.655849679  0.65126386  0.65814259  0.65584968  0.65584968
11   0.10749090  0.097511661  0.10416449  0.10416449  0.10416449  0.10416449
12  -0.39260141 -0.392601414 -0.39558063 -0.39260141 -0.39260141 -0.39260141
13  -0.10667998 -0.106679982 -0.10667998 -0.10667998 -0.10667998 -0.10667998
14  -1.95518028 -1.947792427 -1.95764289 -1.95518028 -1.95518028 -1.95518028
15   0.32697264  0.330282575  0.32697264  0.32697264  0.32697264  0.32697264
16   2.48013666  2.461955765  2.47786405  2.47104621  2.48013666  2.48013666
17  -0.02721152 -0.030216424 -0.03021642 -0.03021642 -0.03021642 -0.03021642
18           NA           NA          NA          NA          NA          NA
19   0.36740533  0.360657461  0.37077926  0.36740533  0.36065746  0.36065746
20   0.68871756  0.677607916  0.68871756  0.68871756  0.68871756  0.68871756
21   2.65042179  2.646373906  2.64637391  2.64637391  2.65042179  2.65042179
22   2.73762932  2.733831920  2.75281890  2.73762932  2.73762932  2.73762932
23   0.13311778  0.111925718  0.12605376  0.12605376  0.11545773  0.11545773
24           NA           NA          NA          NA          NA          NA
25  -1.09549238 -1.095492377 -1.09549238 -1.09325155 -1.09325155 -1.09325155
26           NA           NA          NA          NA          NA          NA
27           NA           NA          NA          NA          NA          NA
28           NA           NA          NA          NA          NA          NA
29           NA           NA          NA          NA          NA          NA
30           NA           NA          NA          NA          NA          NA
31   1.11137037  1.107560577  1.11518017  1.10756058  1.11137037  1.11137037
32   1.10068807  1.100688066  1.09335835  1.09702321  1.09702321  1.09702321
33   0.55977697  0.541627447  0.55977697  0.55977697  0.55977697  0.55977697
34  -0.92797866 -0.931805750 -0.92797866 -0.92797866 -0.92797866 -0.92797866
35   1.87038310  1.859586885  1.87038310  1.87038310  1.87038310  1.87038310
36  -1.50997191 -1.517994857 -1.50997191 -1.51264622 -1.51532054 -1.51532054
37  -0.56710898 -0.583428637 -0.57254887 -0.57798875 -0.57798875 -0.57798875
38   1.41031183  1.407570262  1.41305339  1.40757026  1.41031183  1.41305339
39           NA           NA          NA          NA          NA          NA
40           NA           NA          NA          NA          NA          NA
41  -0.16666843 -0.183668200 -0.16666843 -0.16666843 -0.16666843 -0.16666843
42  -0.75594056 -0.774693215 -0.77156777 -0.77156777 -0.77156777 -0.77156777
43  -0.50061932 -0.510458925 -0.50389919 -0.50389919 -0.50061932 -0.50061932
44   1.13761779  1.137617791  1.12050183  1.12734822  1.13761779  1.13761779
45   2.77970922  2.797994911  2.78702349  2.77970922  2.78336636  2.78336636
46  -1.10878258 -1.096936966 -1.11115170 -1.10641345 -1.10878258 -1.10878258
47   1.09776669  1.090778991  1.10824823  1.10126053  1.10126053  1.10126053
48           NA           NA          NA          NA          NA          NA
49           NA           NA          NA          NA          NA          NA
50           NA           NA          NA          NA          NA          NA
51           NA           NA          NA          NA          NA          NA
52           NA           NA          NA          NA          NA          NA
53   0.29617712  0.296177120  0.29617712  0.29617712  0.29617712  0.29617712
54   0.76638435  0.757704463  0.76638435  0.76204441  0.76204441  0.76204441
55   0.81472351  0.800891541  0.81818151  0.81126552  0.81126552  0.81126552
56           NA           NA          NA          NA          NA          NA
57   0.08446902  0.066734311  0.08446902  0.09333637  0.08446902  0.08446902
58   0.47259013  0.458587127  0.47259013  0.47259013  0.47259013  0.47259013
59   0.81487725  0.799089996  0.81882406  0.81487725  0.81487725  0.81487725
60   0.01195532  0.003903587  0.01195532  0.01195532  0.01195532  0.01195532
61  -0.06583842 -0.055892667 -0.06252317 -0.05589267 -0.06252317 -0.06252317
62   0.27571131  0.278424357  0.27842436  0.27299826  0.27571131  0.27571131
63   0.19587669  0.218934622  0.19971968  0.19971968  0.19971968  0.19971968
64   1.18886009  1.185108231  1.20011567  1.18886009  1.18886009  1.19261195
65   0.77410422  0.763954564  0.77072100  0.77748744  0.77748744  0.77748744
66   0.56889458  0.568894580  0.56889458  0.56889458  0.56889458  0.56889458
67   0.37139283  0.377990280  0.37469156  0.37799028  0.37469156  0.37469156
68   0.46108752  0.461087521  0.46108752  0.46108752  0.46108752  0.46108752
69   1.22250730  1.204838793  1.22250730  1.21897360  1.22250730  1.22250730
70   0.57830868  0.590021204  0.58611703  0.57830868  0.58221285  0.58221285
71  -0.50721383 -0.518366498 -0.51279017 -0.50721383 -0.51000200 -0.51000200
72  -0.20563748 -0.215006194 -0.20563748 -0.21032184 -0.20563748 -0.20563748
73   1.47447431  1.458856017  1.47447431  1.47447431  1.47447431  1.47447431
74   0.87018604  0.866422649  0.87018604  0.87018604  0.87018604  0.87018604
75   1.05594815  1.058671367  1.05867137  1.05594815  1.05867137  1.05867137
76  -0.48777247 -0.487772474 -0.50385322 -0.48777247 -0.48777247 -0.48777247
77  -0.25164229 -0.251642291 -0.25164229 -0.25164229 -0.25164229 -0.25164229
78           NA           NA          NA          NA          NA          NA
79           NA           NA          NA          NA          NA          NA
80   1.27516713  1.279184725  1.27918472  1.27918472  1.27918472  1.27918472
81  -1.00461982 -0.997859864 -1.00123984 -1.00123984 -1.00123984 -1.00123984
82   0.58062313  0.586803375  0.58680338  0.58680338  0.58680338  0.58680338
83  -0.07143937 -0.057914451 -0.06467691 -0.06805814 -0.07143937 -0.07143937
84   1.18320879  1.174880993  1.17904489  1.18320879  1.18320879  1.18320879
85           NA           NA          NA          NA          NA          NA
86           NA           NA          NA          NA          NA          NA
87  -0.30726684 -0.307266841 -0.30103274 -0.30726684 -0.30726684 -0.30726684
88   1.41234087  1.408922131  1.40892213  1.41234087  1.41234087  1.41234087
89  -0.65428510 -0.632113308 -0.64042773 -0.65428510 -0.64874215 -0.64874215
90  -0.94391940 -0.935354600 -0.93535460 -0.93820953 -0.94391940 -0.94391940
91   0.24418264  0.249818154  0.24981815  0.24981815  0.24418264  0.24418264
92  -0.49640909 -0.492209483 -0.48800987 -0.49640909 -0.49640909 -0.49640909
93   1.05225694  1.058852321  1.05225694  1.05225694  1.05225694  1.05225694
94           NA           NA          NA          NA          NA          NA
95           NA           NA          NA          NA          NA          NA
96  -0.40637805 -0.406378045 -0.40637805 -0.40637805 -0.40637805 -0.40637805
97   1.50196696  1.501966964  1.50196696  1.51034838  1.50615767  1.50615767
98           NA           NA          NA          NA          NA          NA
99   0.97747606  0.969652277  0.98529984  0.97747606  0.97747606  0.97747606
100 -0.52606668 -0.533389425 -0.52972805 -0.52972805 -0.52972805 -0.52972805
101 -1.52828543 -1.531086848 -1.51427833 -1.52548401 -1.52548401 -1.52548401
102 -0.74911369 -0.735726982 -0.74911369 -0.74375901 -0.75446837 -0.75446837
103          NA           NA          NA          NA          NA          NA
104 -0.55221197 -0.552211965 -0.55667457 -0.54774937 -0.55221197 -0.55221197
105 -0.66416111 -0.657122443 -0.66181489 -0.66181489 -0.66416111 -0.66416111
106 -0.48155015 -0.470569197 -0.48155015 -0.48521047 -0.48155015 -0.48155015
107          NA           NA          NA          NA          NA          NA
108  2.18012330  2.180123300  2.18012330  2.18012330  2.18012330  2.18012330
       NP_112598    NP_001611
1             NA           NA
2   -2.652150067 -0.984373265
3             NA           NA
4   -1.035759872 -0.517225683
5             NA           NA
6    2.244584354 -2.575064764
7   -0.148204900  0.267490247
8             NA           NA
9   -0.967196074  2.838370489
10  -1.969533682  1.307036469
11  -0.880454146 -1.512472865
12  -2.504861658  0.694810418
13   0.025188599 -1.177327273
14  -1.004610594 -2.551133287
15   1.932290343 -1.910542340
16   3.959606744 -1.858278696
17  -2.551331119 -2.100595477
18            NA           NA
19  -1.650206386  0.401144652
20  -2.733052927 -2.133132127
21   3.909312755 -1.045293501
22   4.089502075 -1.120524403
23  -0.725160560 -2.360481012
24            NA           NA
25   0.096627356 -1.149272214
26            NA           NA
27            NA           NA
28            NA           NA
29            NA           NA
30            NA           NA
31  -1.517390359  0.482753678
32  -2.413909374  0.543629869
33  -1.698023696  1.754015586
34  -3.071151471 -2.278942948
35   1.539299286  1.377356118
36  -2.194596901  0.134732665
37   0.730303716  1.638764597
38   3.195070488 -0.007077155
39            NA           NA
40            NA           NA
41   2.425796388  0.470822918
42   2.363250927  0.025420032
43  -1.845365554 -0.405503121
44   0.589907135  2.257001446
45   2.205538366  0.749996978
46  -1.324372713  1.075548247
47  -3.444234673  2.103994679
48            NA           NA
49            NA           NA
50            NA           NA
51            NA           NA
52            NA           NA
53  -3.254639306  2.351713708
54  -0.583338181  2.185545984
55   1.381834372  1.565108003
56            NA           NA
57  -0.237711460  1.305207881
58  -0.742870654  1.811277356
59  -1.537423401  1.051686039
60   0.225326195  0.325972835
61  -2.187599782  0.709930573
62  -0.001019793  0.131919657
63  -2.624877344  0.653192395
64   1.046289383  2.138080861
65  -2.494085562  1.927781982
66   3.263024321  0.887747701
67  -0.242169869  0.051416585
68   2.318731725 -0.283625077
69  -0.473668945  1.741961269
70  -1.627550800 -1.697825970
71  -2.439413060 -2.174537264
72  -1.151877145  0.272166701
73  -0.352866070  2.813743010
74   1.920170648  2.349196620
75  -0.332890890 -1.250613945
76  -1.626289437  0.731148229
77  -1.613877425 -0.646590070
78            NA           NA
79            NA           NA
80   3.026839582  0.961794533
81  -1.379797505  0.563690494
82   0.052212450  1.501479177
83   0.351214199 -0.287837990
84   4.955701511  0.825113456
85            NA           NA
86            NA           NA
87  -1.460574599  1.170213908
88   1.008929830  1.802076956
89  -0.618255939  1.222002715
90  -1.252252127  1.325752082
91  -1.860681139 -0.229200377
92  -1.300634341  1.349319349
93   0.679617968  3.436486828
94            NA           NA
95            NA           NA
96   2.258487365  0.972084153
97   2.151527016  1.552255484
98            NA           NA
99  -1.811702422  3.262020566
100 -4.952665524 -0.526066681
101 -3.147505566 -1.738391848
102 -0.700921541  0.726101526
103           NA           NA
104  0.679465615  0.487573818
105  0.234441727  0.980540164
106 -2.802192305 -0.997655112
107           NA           NA
108  1.911710702  0.896216374
```
]

---
count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
brca %&gt;%
* select(ER.Status, starts_with('NP'))
```
]
 
.panel2-through-auto[

```
    ER.Status   NP_958782   NP_958785   NP_958786    NP_000436   NP_958781
1    Negative          NA          NA          NA           NA          NA
2    Negative  0.68340354  0.69442409  0.69809760  0.687077054  0.68707705
3    Negative          NA          NA          NA           NA          NA
4    Negative  0.19534065  0.21541294  0.21541294  0.205376799  0.21541294
5    Negative          NA          NA          NA           NA          NA
6    Negative -1.12317308 -1.12317308 -1.11686051 -1.129485657 -1.12948566
7    Negative  0.53859578  0.54221052  0.54221052  0.534981038  0.54221052
8    Negative          NA          NA          NA           NA          NA
9    Negative  0.83113175  0.85653983  0.85653983  0.836777987  0.86500919
10   Negative  0.65584968  0.65814259  0.65584968  0.655849679  0.65126386
11   Negative  0.10749090  0.10416449  0.10749090  0.097511661  0.10416449
12   Negative -0.39855983 -0.39260141 -0.39260141 -0.392601414 -0.39558063
13   Negative -0.10667998 -0.10667998 -0.10667998 -0.106679982 -0.10667998
14   Negative -1.94779243 -1.95271766 -1.95518028 -1.947792427 -1.95764289
15   Negative  0.32366271  0.32697264  0.32697264  0.330282575  0.32697264
16   Negative  2.45513793  2.48013666  2.48013666  2.461955765  2.47786405
17   Negative -0.03322133 -0.03021642 -0.02721152 -0.030216424 -0.03021642
18   Negative          NA          NA          NA           NA          NA
19   Negative  0.35053566  0.36740533  0.36740533  0.360657461  0.37077926
20   Negative  0.67390470  0.68871756  0.68871756  0.677607916  0.68871756
21   Negative  2.60994298  2.65042179  2.65042179  2.646373906  2.64637391
22   Negative  2.70725015  2.73383192  2.73762932  2.733831920  2.75281890
23   Negative  0.14018179  0.12605376  0.13311778  0.111925718  0.12605376
24   Negative          NA          NA          NA           NA          NA
25   Negative -1.08652907 -1.09549238 -1.09549238 -1.095492377 -1.09549238
26   Negative          NA          NA          NA           NA          NA
27       &lt;NA&gt;          NA          NA          NA           NA          NA
28   Negative          NA          NA          NA           NA          NA
29   Negative          NA          NA          NA           NA          NA
30   Negative          NA          NA          NA           NA          NA
31   Negative  1.09613118  1.11137037  1.11137037  1.107560577  1.11518017
32   Negative  1.10068807  1.10068807  1.10068807  1.100688066  1.09335835
33   Negative  0.55977697  0.56340687  0.55977697  0.541627447  0.55977697
34   Negative -0.91267028 -0.92797866 -0.92797866 -0.931805750 -0.92797866
35   Positive  1.87398183  1.87038310  1.87038310  1.859586885  1.87038310
36   Positive -1.52334349 -1.51264622 -1.50997191 -1.517994857 -1.50997191
37   Positive -0.58342864 -0.57254887 -0.56710898 -0.583428637 -0.57254887
38   Negative  1.40757026  1.40757026  1.41031183  1.407570262  1.41305339
39   Negative          NA          NA          NA           NA          NA
40   Negative          NA          NA          NA           NA          NA
41   Positive -0.20491791 -0.16241849 -0.16666843 -0.183668200 -0.16666843
42   Negative -0.78719498 -0.75594056 -0.75594056 -0.774693215 -0.77156777
43   Positive -0.49405958 -0.50389919 -0.50061932 -0.510458925 -0.50389919
44   Negative  1.12050183  1.13761779  1.13761779  1.137617791  1.12050183
45   Positive  2.76508066  2.77970922  2.77970922  2.797994911  2.78702349
46   Positive -1.10167521 -1.10878258 -1.10878258 -1.096936966 -1.11115170
47   Positive  1.10126053  1.10126053  1.09776669  1.090778991  1.10824823
48   Positive          NA          NA          NA           NA          NA
49   Positive          NA          NA          NA           NA          NA
50   Positive          NA          NA          NA           NA          NA
51   Positive          NA          NA          NA           NA          NA
52   Positive          NA          NA          NA           NA          NA
53   Positive  0.31131919  0.29617712  0.29617712  0.296177120  0.29617712
54   Positive  0.76204441  0.76204441  0.76638435  0.757704463  0.76638435
55   Positive  0.79397556  0.81818151  0.81472351  0.800891541  0.81818151
56   Positive          NA          NA          NA           NA          NA
57   Positive  0.06377853  0.09333637  0.08446902  0.066734311  0.08446902
58   Positive  0.45298593  0.47259013  0.47259013  0.458587127  0.47259013
59   Positive  0.81882406  0.81487725  0.81487725  0.799089996  0.81882406
60   Positive  0.02000705  0.01195532  0.01195532  0.003903587  0.01195532
61   Positive -0.06583842 -0.05589267 -0.06583842 -0.055892667 -0.06252317
62   Positive  0.26485911  0.27571131  0.27571131  0.278424357  0.27842436
63   Positive  0.19587669  0.19587669  0.19587669  0.218934622  0.19971968
64   Positive  1.18510823  1.19261195  1.18886009  1.185108231  1.20011567
65   Positive  0.75718813  0.78087066  0.77410422  0.763954564  0.77072100
66   Positive  0.56889458  0.56889458  0.56889458  0.568894580  0.56889458
67   Positive  0.38458773  0.37139283  0.37139283  0.377990280  0.37469156
68   Positive  0.46108752  0.46108752  0.46108752  0.461087521  0.46108752
69   Positive  1.22250730  1.21897360  1.22250730  1.204838793  1.22250730
70   Positive  0.57830868  0.58221285  0.57830868  0.590021204  0.58611703
71   Positive -0.51836650 -0.51000200 -0.50721383 -0.518366498 -0.51279017
72   Positive -0.20563748 -0.20563748 -0.20563748 -0.215006194 -0.20563748
73   Positive  1.46666516  1.48228346  1.47447431  1.458856017  1.47447431
74   Positive  0.86265926  0.87018604  0.87018604  0.866422649  0.87018604
75   Positive  1.05322494  1.05594815  1.05594815  1.058671367  1.05867137
76   Positive -0.48777247 -0.48777247 -0.48777247 -0.487772474 -0.50385322
77   Positive -0.26503035 -0.25164229 -0.25164229 -0.251642291 -0.25164229
78   Positive          NA          NA          NA           NA          NA
79   Positive          NA          NA          NA           NA          NA
80   Positive  1.27918472  1.27516713  1.27516713  1.279184725  1.27918472
81   Positive -1.00123984 -1.00461982 -1.00461982 -0.997859864 -1.00123984
82   Positive  0.58371325  0.58062313  0.58062313  0.586803375  0.58680338
83   Positive -0.07820182 -0.06805814 -0.07143937 -0.057914451 -0.06467691
84   Positive  1.17488099  1.18320879  1.18320879  1.174880993  1.17904489
85   Positive          NA          NA          NA           NA          NA
86   Positive          NA          NA          NA           NA          NA
87   Positive -0.30726684 -0.30726684 -0.30726684 -0.307266841 -0.30103274
88   Positive  1.39524718  1.40892213  1.41234087  1.408922131  1.40892213
89   Positive -0.65982805 -0.64874215 -0.65428510 -0.632113308 -0.64042773
90   Positive -0.96390393 -0.93820953 -0.94391940 -0.935354600 -0.93535460
91   Positive  0.23854713  0.24981815  0.24418264  0.249818154  0.24981815
92   Positive -0.49640909 -0.49850890 -0.49640909 -0.492209483 -0.48800987
93   Positive  1.04895925  1.05225694  1.05225694  1.058852321  1.05225694
94   Positive          NA          NA          NA           NA          NA
95   Positive          NA          NA          NA           NA          NA
96   Positive -0.42818149 -0.40637805 -0.40637805 -0.406378045 -0.40637805
97   Positive  1.50196696  1.51034838  1.50196696  1.501966964  1.50196696
98   Positive          NA          NA          NA           NA          NA
99   Positive  0.97356417  0.97747606  0.97747606  0.969652277  0.98529984
100  Positive -0.51142119 -0.52606668 -0.52606668 -0.533389425 -0.52972805
101  Positive -1.51427833 -1.52828543 -1.52828543 -1.531086848 -1.51427833
102  Positive -0.75982306 -0.75982306 -0.74911369 -0.735726982 -0.74911369
103  Positive          NA          NA          NA           NA          NA
104  Positive -0.55221197 -0.54774937 -0.55221197 -0.552211965 -0.55667457
105  Positive -0.69231577 -0.65946866 -0.66416111 -0.657122443 -0.66181489
106  Positive -0.48155015 -0.47788983 -0.48155015 -0.470569197 -0.48155015
107  Positive          NA          NA          NA           NA          NA
108  Positive  2.18012330  2.18012330  2.18012330  2.180123300  2.18012330
      NP_958780   NP_958783   NP_958784    NP_112598    NP_001611
1            NA          NA          NA           NA           NA
2    0.69809760  0.69809760  0.69809760 -2.652150067 -0.984373265
3            NA          NA          NA           NA           NA
4    0.21541294  0.21541294  0.21541294 -1.035759872 -0.517225683
5            NA          NA          NA           NA           NA
6   -1.12001680 -1.12317308 -1.12317308  2.244584354 -2.575064764
7    0.54221052  0.54221052  0.54221052 -0.148204900  0.267490247
8            NA          NA          NA           NA           NA
9    0.85653983  0.85089359  0.85089359 -0.967196074  2.838370489
10   0.65814259  0.65584968  0.65584968 -1.969533682  1.307036469
11   0.10416449  0.10416449  0.10416449 -0.880454146 -1.512472865
12  -0.39260141 -0.39260141 -0.39260141 -2.504861658  0.694810418
13  -0.10667998 -0.10667998 -0.10667998  0.025188599 -1.177327273
14  -1.95518028 -1.95518028 -1.95518028 -1.004610594 -2.551133287
15   0.32697264  0.32697264  0.32697264  1.932290343 -1.910542340
16   2.47104621  2.48013666  2.48013666  3.959606744 -1.858278696
17  -0.03021642 -0.03021642 -0.03021642 -2.551331119 -2.100595477
18           NA          NA          NA           NA           NA
19   0.36740533  0.36065746  0.36065746 -1.650206386  0.401144652
20   0.68871756  0.68871756  0.68871756 -2.733052927 -2.133132127
21   2.64637391  2.65042179  2.65042179  3.909312755 -1.045293501
22   2.73762932  2.73762932  2.73762932  4.089502075 -1.120524403
23   0.12605376  0.11545773  0.11545773 -0.725160560 -2.360481012
24           NA          NA          NA           NA           NA
25  -1.09325155 -1.09325155 -1.09325155  0.096627356 -1.149272214
26           NA          NA          NA           NA           NA
27           NA          NA          NA           NA           NA
28           NA          NA          NA           NA           NA
29           NA          NA          NA           NA           NA
30           NA          NA          NA           NA           NA
31   1.10756058  1.11137037  1.11137037 -1.517390359  0.482753678
32   1.09702321  1.09702321  1.09702321 -2.413909374  0.543629869
33   0.55977697  0.55977697  0.55977697 -1.698023696  1.754015586
34  -0.92797866 -0.92797866 -0.92797866 -3.071151471 -2.278942948
35   1.87038310  1.87038310  1.87038310  1.539299286  1.377356118
36  -1.51264622 -1.51532054 -1.51532054 -2.194596901  0.134732665
37  -0.57798875 -0.57798875 -0.57798875  0.730303716  1.638764597
38   1.40757026  1.41031183  1.41305339  3.195070488 -0.007077155
39           NA          NA          NA           NA           NA
40           NA          NA          NA           NA           NA
41  -0.16666843 -0.16666843 -0.16666843  2.425796388  0.470822918
42  -0.77156777 -0.77156777 -0.77156777  2.363250927  0.025420032
43  -0.50389919 -0.50061932 -0.50061932 -1.845365554 -0.405503121
44   1.12734822  1.13761779  1.13761779  0.589907135  2.257001446
45   2.77970922  2.78336636  2.78336636  2.205538366  0.749996978
46  -1.10641345 -1.10878258 -1.10878258 -1.324372713  1.075548247
47   1.10126053  1.10126053  1.10126053 -3.444234673  2.103994679
48           NA          NA          NA           NA           NA
49           NA          NA          NA           NA           NA
50           NA          NA          NA           NA           NA
51           NA          NA          NA           NA           NA
52           NA          NA          NA           NA           NA
53   0.29617712  0.29617712  0.29617712 -3.254639306  2.351713708
54   0.76204441  0.76204441  0.76204441 -0.583338181  2.185545984
55   0.81126552  0.81126552  0.81126552  1.381834372  1.565108003
56           NA          NA          NA           NA           NA
57   0.09333637  0.08446902  0.08446902 -0.237711460  1.305207881
58   0.47259013  0.47259013  0.47259013 -0.742870654  1.811277356
59   0.81487725  0.81487725  0.81487725 -1.537423401  1.051686039
60   0.01195532  0.01195532  0.01195532  0.225326195  0.325972835
61  -0.05589267 -0.06252317 -0.06252317 -2.187599782  0.709930573
62   0.27299826  0.27571131  0.27571131 -0.001019793  0.131919657
63   0.19971968  0.19971968  0.19971968 -2.624877344  0.653192395
64   1.18886009  1.18886009  1.19261195  1.046289383  2.138080861
65   0.77748744  0.77748744  0.77748744 -2.494085562  1.927781982
66   0.56889458  0.56889458  0.56889458  3.263024321  0.887747701
67   0.37799028  0.37469156  0.37469156 -0.242169869  0.051416585
68   0.46108752  0.46108752  0.46108752  2.318731725 -0.283625077
69   1.21897360  1.22250730  1.22250730 -0.473668945  1.741961269
70   0.57830868  0.58221285  0.58221285 -1.627550800 -1.697825970
71  -0.50721383 -0.51000200 -0.51000200 -2.439413060 -2.174537264
72  -0.21032184 -0.20563748 -0.20563748 -1.151877145  0.272166701
73   1.47447431  1.47447431  1.47447431 -0.352866070  2.813743010
74   0.87018604  0.87018604  0.87018604  1.920170648  2.349196620
75   1.05594815  1.05867137  1.05867137 -0.332890890 -1.250613945
76  -0.48777247 -0.48777247 -0.48777247 -1.626289437  0.731148229
77  -0.25164229 -0.25164229 -0.25164229 -1.613877425 -0.646590070
78           NA          NA          NA           NA           NA
79           NA          NA          NA           NA           NA
80   1.27918472  1.27918472  1.27918472  3.026839582  0.961794533
81  -1.00123984 -1.00123984 -1.00123984 -1.379797505  0.563690494
82   0.58680338  0.58680338  0.58680338  0.052212450  1.501479177
83  -0.06805814 -0.07143937 -0.07143937  0.351214199 -0.287837990
84   1.18320879  1.18320879  1.18320879  4.955701511  0.825113456
85           NA          NA          NA           NA           NA
86           NA          NA          NA           NA           NA
87  -0.30726684 -0.30726684 -0.30726684 -1.460574599  1.170213908
88   1.41234087  1.41234087  1.41234087  1.008929830  1.802076956
89  -0.65428510 -0.64874215 -0.64874215 -0.618255939  1.222002715
90  -0.93820953 -0.94391940 -0.94391940 -1.252252127  1.325752082
91   0.24981815  0.24418264  0.24418264 -1.860681139 -0.229200377
92  -0.49640909 -0.49640909 -0.49640909 -1.300634341  1.349319349
93   1.05225694  1.05225694  1.05225694  0.679617968  3.436486828
94           NA          NA          NA           NA           NA
95           NA          NA          NA           NA           NA
96  -0.40637805 -0.40637805 -0.40637805  2.258487365  0.972084153
97   1.51034838  1.50615767  1.50615767  2.151527016  1.552255484
98           NA          NA          NA           NA           NA
99   0.97747606  0.97747606  0.97747606 -1.811702422  3.262020566
100 -0.52972805 -0.52972805 -0.52972805 -4.952665524 -0.526066681
101 -1.52548401 -1.52548401 -1.52548401 -3.147505566 -1.738391848
102 -0.74375901 -0.75446837 -0.75446837 -0.700921541  0.726101526
103          NA          NA          NA           NA           NA
104 -0.54774937 -0.55221197 -0.55221197  0.679465615  0.487573818
105 -0.66181489 -0.66416111 -0.66416111  0.234441727  0.980540164
106 -0.48521047 -0.48155015 -0.48155015 -2.802192305 -0.997655112
107          NA          NA          NA           NA           NA
108  2.18012330  2.18012330  2.18012330  1.911710702  0.896216374
```
]

---
count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
brca %&gt;%
  select(ER.Status, starts_with('NP')) %&gt;%
* pivot_longer(names_to = 'protein',
*              values_to = 'expression',
*              cols = c(-ER.Status))
```
]
 
.panel2-through-auto[

```
# A tibble: 1,080 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958782         NA
2 Negative  NP_958785         NA
3 Negative  NP_958786         NA
# … with 1,077 more rows
```
]

---
count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
brca %&gt;%
  select(ER.Status, starts_with('NP')) %&gt;%
  pivot_longer(names_to = 'protein',
               values_to = 'expression',
               cols = c(-ER.Status)) %&gt;%
* split(.$protein)
```
]
 
.panel2-through-auto[

```
$NP_000436
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_000436     NA    
2 Negative  NP_000436      0.687
3 Negative  NP_000436     NA    
# … with 105 more rows

$NP_001611
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_001611     NA    
2 Negative  NP_001611     -0.984
3 Negative  NP_001611     NA    
# … with 105 more rows

$NP_112598
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_112598      NA   
2 Negative  NP_112598      -2.65
3 Negative  NP_112598      NA   
# … with 105 more rows

$NP_958780
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958780     NA    
2 Negative  NP_958780      0.698
3 Negative  NP_958780     NA    
# … with 105 more rows

$NP_958781
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958781     NA    
2 Negative  NP_958781      0.687
3 Negative  NP_958781     NA    
# … with 105 more rows

$NP_958782
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958782     NA    
2 Negative  NP_958782      0.683
3 Negative  NP_958782     NA    
# … with 105 more rows

$NP_958783
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958783     NA    
2 Negative  NP_958783      0.698
3 Negative  NP_958783     NA    
# … with 105 more rows

$NP_958784
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958784     NA    
2 Negative  NP_958784      0.698
3 Negative  NP_958784     NA    
# … with 105 more rows

$NP_958785
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958785     NA    
2 Negative  NP_958785      0.694
3 Negative  NP_958785     NA    
# … with 105 more rows

$NP_958786
# A tibble: 108 x 3
  ER.Status protein   expression
  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;
1 Negative  NP_958786     NA    
2 Negative  NP_958786      0.698
3 Negative  NP_958786     NA    
# … with 105 more rows
```
]

---
count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
brca %&gt;%
  select(ER.Status, starts_with('NP')) %&gt;%
  pivot_longer(names_to = 'protein',
               values_to = 'expression',
               cols = c(-ER.Status)) %&gt;%
  split(.$protein) %&gt;%
* map(~broom::tidy(t.test(expression ~ ER.Status,
*                         data=.)))
```
]
 
.panel2-through-auto[

```
$NP_000436
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.161     0.432     0.271     0.628   0.534      41.5   -0.356     0.677
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_001611
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic  p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    -1.41    -0.566     0.840     -4.10 0.000199      39.9    -2.10    -0.712
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_112598
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.160    -0.197    -0.357     0.306   0.761      43.5   -0.892      1.21
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958780
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.163     0.436     0.273     0.637   0.528      41.6   -0.354     0.680
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958781
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.162     0.436     0.274     0.633   0.530      41.5   -0.356     0.680
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958782
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.162     0.429     0.267     0.635   0.529      41.8   -0.352     0.676
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958783
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.164     0.436     0.272     0.639   0.527      41.5   -0.354     0.681
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958784
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.164     0.436     0.273     0.639   0.527      41.5   -0.354     0.681
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958785
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.165     0.438     0.273     0.642   0.524      41.6   -0.353     0.682
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  

$NP_958786
# A tibble: 1 x 10
  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    0.166     0.439     0.272     0.649   0.520      41.6   -0.351     0.684
  method                  alternative
  &lt;chr&gt;                   &lt;chr&gt;      
1 Welch Two Sample t-test two.sided  
```
]

---
count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
brca %&gt;%
  select(ER.Status, starts_with('NP')) %&gt;%
  pivot_longer(names_to = 'protein',
               values_to = 'expression',
               cols = c(-ER.Status)) %&gt;%
  split(.$protein) %&gt;%
  map(~broom::tidy(t.test(expression ~ ER.Status,
                          data=.))) %&gt;%
* bind_rows(.id = 'Protein')
```
]
 
.panel2-through-auto[

```
# A tibble: 10 x 11
   Protein   estimate estimate1 estimate2 statistic  p.value parameter conf.low
   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
 1 NP_000436    0.161     0.432     0.271     0.628 0.534         41.5   -0.356
 2 NP_001611   -1.41     -0.566     0.840    -4.10  0.000199      39.9   -2.10 
 3 NP_112598    0.160    -0.197    -0.357     0.306 0.761         43.5   -0.892
 4 NP_958780    0.163     0.436     0.273     0.637 0.528         41.6   -0.354
 5 NP_958781    0.162     0.436     0.274     0.633 0.530         41.5   -0.356
 6 NP_958782    0.162     0.429     0.267     0.635 0.529         41.8   -0.352
 7 NP_958783    0.164     0.436     0.272     0.639 0.527         41.5   -0.354
 8 NP_958784    0.164     0.436     0.273     0.639 0.527         41.5   -0.354
 9 NP_958785    0.165     0.438     0.273     0.642 0.524         41.6   -0.353
10 NP_958786    0.166     0.439     0.272     0.649 0.520         41.6   -0.351
   conf.high method                  alternative
       &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;      
 1     0.677 Welch Two Sample t-test two.sided  
 2    -0.712 Welch Two Sample t-test two.sided  
 3     1.21  Welch Two Sample t-test two.sided  
 4     0.680 Welch Two Sample t-test two.sided  
 5     0.680 Welch Two Sample t-test two.sided  
 6     0.676 Welch Two Sample t-test two.sided  
 7     0.681 Welch Two Sample t-test two.sided  
 8     0.681 Welch Two Sample t-test two.sided  
 9     0.682 Welch Two Sample t-test two.sided  
10     0.684 Welch Two Sample t-test two.sided  
```
]

---
count: false
 
## Using &lt;code&gt;broom&lt;/code&gt;
The fact that &lt;code&gt;broom::tidy&lt;/code&gt; makes the results of tests into tibbles is in fact extremely useful in high-throughput work
.panel1-through-auto[

```r
brca %&gt;%
  select(ER.Status, starts_with('NP')) %&gt;%
  pivot_longer(names_to = 'protein',
               values_to = 'expression',
               cols = c(-ER.Status)) %&gt;%
  split(.$protein) %&gt;%
  map(~broom::tidy(t.test(expression ~ ER.Status,
                          data=.))) %&gt;%
  bind_rows(.id = 'Protein') %&gt;%
* select(Protein, estimate, p.value, conf.low, conf.high)
```
]
 
.panel2-through-auto[

```
# A tibble: 10 x 5
   Protein   estimate  p.value conf.low conf.high
   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
 1 NP_000436    0.161 0.534      -0.356     0.677
 2 NP_001611   -1.41  0.000199   -2.10     -0.712
 3 NP_112598    0.160 0.761      -0.892     1.21 
 4 NP_958780    0.163 0.528      -0.354     0.680
 5 NP_958781    0.162 0.530      -0.356     0.680
 6 NP_958782    0.162 0.529      -0.352     0.676
 7 NP_958783    0.164 0.527      -0.354     0.681
 8 NP_958784    0.164 0.527      -0.354     0.681
 9 NP_958785    0.165 0.524      -0.353     0.682
10 NP_958786    0.166 0.520      -0.351     0.684
```
]

&lt;style&gt;
.panel1-through-auto {
  color: black;
  width: 58.8%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel2-through-auto {
  color: black;
  width: 39.2%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel3-through-auto {
  color: black;
  width: NA%;
  hight: 33%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
&lt;/style&gt;




---
class: center, middle

# Back to testing

---

## Wilcoxon test, nonparametric t-test


```r
wilcox.test(NP_958782 ~ ER.Status, data=brca) %&gt;% 
  broom::tidy()
```

```
# A tibble: 1 x 4
  statistic p.value method                                           
      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            
1       755   0.590 Wilcoxon rank sum test with continuity correction
  alternative
  &lt;chr&gt;      
1 two.sided  
```

--


```

	Wilcoxon rank sum test with continuity correction

data:  NP_958782 by ER.Status
W = 755, p-value = 0.5897
alternative hypothesis: true location shift is not equal to 0
```

---

## Wilcoxon test

.pull-left[

```r
brca %&gt;% 
  select(ER.Status, starts_with('NP')) %&gt;% 
  tidyr::gather(protein,expression, -ER.Status) %&gt;% 
  split(.$protein) %&gt;% 
  map(~broom::tidy(wilcox.test(expression ~ ER.Status, 
                          data=.))) %&gt;% 
  bind_rows(.id='Protein') %&gt;% 
  select(Protein, p.value)
```
]
.pull-right[

```
# A tibble: 10 x 2
   Protein     p.value
   &lt;chr&gt;         &lt;dbl&gt;
 1 NP_000436 0.583    
 2 NP_001611 0.0000928
 3 NP_112598 0.939    
 4 NP_958780 0.583    
 5 NP_958781 0.576    
 6 NP_958782 0.590    
 7 NP_958783 0.583    
 8 NP_958784 0.576    
 9 NP_958785 0.576    
10 NP_958786 0.576    
```
]

---

## Using `tableone`



```r
CreateTableOne(
  data = brca %&gt;% filter(!is.na(ER.Status)),
  vars = brca %&gt;% 
    select(starts_with('NP')) %&gt;% 
    names(),
  strata = 'ER.Status',
  test = T,
  testNormal = t.test
)
```

```
                       Stratified by ER.Status
                        Negative     Positive     p      test
  n                        38           69                   
  NP_958782 (mean (SD))  0.43 (1.13)  0.27 (0.93)  0.498     
  NP_958785 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.492     
  NP_958786 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.487     
  NP_000436 (mean (SD))  0.43 (1.14)  0.27 (0.93)  0.502     
  NP_958781 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.499     
  NP_958780 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.496     
  NP_958783 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.495     
  NP_958784 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.495     
  NP_112598 (mean (SD)) -0.20 (2.28) -0.36 (1.97)  0.748     
  NP_001611 (mean (SD)) -0.57 (1.54)  0.84 (1.19) &lt;0.001     
```

--

This is not quite the same results as before

---

## Using `tableone`



```r
CreateTableOne(
  data = brca %&gt;% filter(!is.na(ER.Status)),
  vars = brca %&gt;% 
    select(starts_with('NP')) %&gt;% 
    names(),
  strata = 'ER.Status',
  test = T,
  testNormal = t.test,
* argsNormal = list(var.equal=F)
)
```

```
                       Stratified by ER.Status
                        Negative     Positive     p      test
  n                        38           69                   
  NP_958782 (mean (SD))  0.43 (1.13)  0.27 (0.93)  0.529     
  NP_958785 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.524     
  NP_958786 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.520     
  NP_000436 (mean (SD))  0.43 (1.14)  0.27 (0.93)  0.534     
  NP_958781 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.530     
  NP_958780 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.528     
  NP_958783 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.527     
  NP_958784 (mean (SD))  0.44 (1.14)  0.27 (0.93)  0.527     
  NP_112598 (mean (SD)) -0.20 (2.28) -0.36 (1.97)  0.761     
  NP_001611 (mean (SD)) -0.57 (1.54)  0.84 (1.19) &lt;0.001     
```

---

## Tests for discrete data

Testing whether the distribution of a categorical variable differs by levels of 
another categorical variable can be done using either the Chi-square test (`chisq.test`) or the Fisher's test (`fisher.test`). Both require you to create a 2x2 table first.


```r
fisher.test(table(brca$Tumor, brca$ER.Status))
```

```

	Fisher's Exact Test for Count Data

data:  table(brca$Tumor, brca$ER.Status)
p-value = 0.6003
alternative hypothesis: two.sided
```

---

## Tests for discrete data

Testing whether the distribution of a categorical variable differs by levels of 
another categorical variable can be done using either the Chi-square test (`chisq.test`) or the Fisher's test (`fisher.test`). Both require you to create a 2x2 table first.


```r
chisq.test(table(brca$Tumor, brca$ER.Status))
```

```

	Pearson's Chi-squared test

data:  table(brca$Tumor, brca$ER.Status)
X-squared = 2.094, df = 3, p-value = 0.5531
```

---

## Tests for discrete data

We can use `broom::tidy` for either of these


```r
chisq.test(table(brca$Tumor, brca$ER.Status)) %&gt;% 
  broom::tidy()
```

```
# A tibble: 1 x 4
  statistic p.value parameter method                    
      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     
1      2.09   0.553         3 Pearson's Chi-squared test
```

---

## Using `tableone`


```r
CreateCatTable(vars = c('Tumor','Node','Metastasis'),
               data = filter(brca, !is.na(ER.Status)),
               strata = 'ER.Status',
               test = T) # chisq.test
```

```
                     Stratified by ER.Status
                      Negative   Positive   p      test
  n                   38         69                    
  Tumor (%)                                  0.553     
     T1                6 (15.8)  10 (14.5)             
     T2               26 (68.4)  40 (58.0)             
     T3                5 (13.2)  14 (20.3)             
     T4                1 ( 2.6)   5 ( 7.2)             
  Node (%)                                   0.685     
     N0               22 (57.9)  32 (46.4)             
     N1                8 (21.1)  21 (30.4)             
     N2                5 (13.2)  10 (14.5)             
     N3                3 ( 7.9)   6 ( 8.7)             
  Metastasis = M1 (%)  1 ( 2.6)   1 ( 1.4)   1.000     
```

---

## Using `tableone`


```r
c1 &lt;- CreateCatTable(vars = c('Tumor','Node','Metastasis'),
               data = filter(brca, !is.na(ER.Status)),
               strata = 'ER.Status',
               test = T)
print(c1, exact = c('Tumor','Node','Metastasis')) # fisher.test
```

```
                     Stratified by ER.Status
                      Negative   Positive   p      test 
  n                   38         69                     
  Tumor (%)                                  0.600 exact
     T1                6 (15.8)  10 (14.5)              
     T2               26 (68.4)  40 (58.0)              
     T3                5 (13.2)  14 (20.3)              
     T4                1 ( 2.6)   5 ( 7.2)              
  Node (%)                                   0.695 exact
     N0               22 (57.9)  32 (46.4)              
     N1                8 (21.1)  21 (30.4)              
     N2                5 (13.2)  10 (14.5)              
     N3                3 ( 7.9)   6 ( 8.7)              
  Metastasis = M1 (%)  1 ( 2.6)   1 ( 1.4)   1.000 exact
```

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../js/macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "r",
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
